################################################################################
20 NewsGroups:

KerasBlog_CNN_Classifier
filter_sizes=[5,5,5]
filter_counts=[128,128,128]
learning_rate=0.001
batch_size=128
num_epochs=40
Words=5000

RESULTS:
Total Doc Count :  18846
Label dimention :  (11314, 20)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  18846
Num of words per Doc :  12013
Number of unique Words :  4998
Words not found in embeddings :  2109
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4998, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5, 5],  							
Filter-Counts : [128, 128, 128], 						
Pool-Windows : [5, 5, 5]
Nm of classes :  20
Input tensor shape:  (None, 1000)
2018-03-28 21:12:06.495334: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-28 21:12:06.495404: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-28 21:12:06.495452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 1000, 300)
Convolution shape at loop  0  :  (None, 996, 128)
Max-Pool shape at loop  0  :  (None, 199, 128)
Convolution shape at loop  1  :  (None, 195, 128)
Max-Pool shape at loop  1  :  (None, 39, 128)
Convolution shape at loop  2  :  (None, 35, 128)
Max-Pool shape at loop  2  :  (None, 7, 128)
Train on 9051 samples, validate on 2263 samples
Epoch 1/40
 - 67s - loss: 2.9728 - acc: 0.0679 - val_loss: 2.8762 - val_acc: 0.0884
Epoch 2/40
 - 67s - loss: 2.6484 - acc: 0.1248 - val_loss: 2.2032 - val_acc: 0.2762
Epoch 3/40
 - 67s - loss: 1.9970 - acc: 0.2845 - val_loss: 1.5584 - val_acc: 0.4264
Epoch 4/40
 - 67s - loss: 1.4886 - acc: 0.4320 - val_loss: 1.3005 - val_acc: 0.5320
Epoch 5/40
 - 67s - loss: 1.1982 - acc: 0.5555 - val_loss: 1.1320 - val_acc: 0.5868
Epoch 6/40
 - 67s - loss: 0.9782 - acc: 0.6256 - val_loss: 1.0264 - val_acc: 0.6381
Epoch 7/40
 - 67s - loss: 0.8537 - acc: 0.6806 - val_loss: 0.9789 - val_acc: 0.6920
Epoch 8/40
 - 67s - loss: 0.7130 - acc: 0.7390 - val_loss: 1.0219 - val_acc: 0.6982
Epoch 9/40
 - 67s - loss: 0.6073 - acc: 0.7871 - val_loss: 1.0187 - val_acc: 0.7212
Epoch 10/40
 - 67s - loss: 0.4836 - acc: 0.8373 - val_loss: 0.8684 - val_acc: 0.7627
Epoch 11/40
 - 66s - loss: 0.4065 - acc: 0.8673 - val_loss: 0.8147 - val_acc: 0.7941
Epoch 12/40
 - 67s - loss: 0.3469 - acc: 0.8895 - val_loss: 0.8308 - val_acc: 0.7817
Epoch 13/40
 - 66s - loss: 0.2907 - acc: 0.9047 - val_loss: 0.8435 - val_acc: 0.7998
Epoch 14/40
 - 67s - loss: 0.2413 - acc: 0.9241 - val_loss: 0.8425 - val_acc: 0.8118
Epoch 15/40
 - 67s - loss: 0.2097 - acc: 0.9360 - val_loss: 0.8859 - val_acc: 0.8047
Epoch 16/40
 - 67s - loss: 0.1864 - acc: 0.9453 - val_loss: 0.8780 - val_acc: 0.8060
Epoch 17/40
 - 67s - loss: 0.1535 - acc: 0.9514 - val_loss: 1.0010 - val_acc: 0.7976
Epoch 18/40
 - 67s - loss: 0.1496 - acc: 0.9576 - val_loss: 0.9715 - val_acc: 0.8016
Epoch 19/40
 - 67s - loss: 0.1413 - acc: 0.9582 - val_loss: 1.0071 - val_acc: 0.8175
Epoch 20/40
 - 67s - loss: 0.1384 - acc: 0.9580 - val_loss: 0.9922 - val_acc: 0.8051
Epoch 21/40
 - 67s - loss: 0.1483 - acc: 0.9540 - val_loss: 0.9122 - val_acc: 0.8237
Epoch 22/40
 - 67s - loss: 0.0968 - acc: 0.9737 - val_loss: 1.0006 - val_acc: 0.8171
Epoch 23/40
 - 67s - loss: 0.0995 - acc: 0.9753 - val_loss: 1.0042 - val_acc: 0.8277
Epoch 24/40
 - 67s - loss: 0.0843 - acc: 0.9772 - val_loss: 1.0654 - val_acc: 0.8144
Epoch 25/40
 - 67s - loss: 0.0881 - acc: 0.9745 - val_loss: 1.1007 - val_acc: 0.8188
Epoch 26/40
 - 67s - loss: 0.0759 - acc: 0.9790 - val_loss: 1.1218 - val_acc: 0.8162
Epoch 27/40
 - 67s - loss: 0.0696 - acc: 0.9793 - val_loss: 1.2956 - val_acc: 0.8003
Epoch 28/40
 - 68s - loss: 0.0724 - acc: 0.9779 - val_loss: 1.2382 - val_acc: 0.8091
Epoch 29/40
 - 67s - loss: 0.0641 - acc: 0.9821 - val_loss: 1.1315 - val_acc: 0.8312
Epoch 30/40
 - 67s - loss: 0.0790 - acc: 0.9774 - val_loss: 1.0880 - val_acc: 0.8263
Epoch 31/40
 - 67s - loss: 0.0517 - acc: 0.9854 - val_loss: 1.1405 - val_acc: 0.8281
Epoch 32/40
 - 67s - loss: 0.0645 - acc: 0.9815 - val_loss: 1.2242 - val_acc: 0.8219
Epoch 33/40
 - 67s - loss: 0.0539 - acc: 0.9849 - val_loss: 1.3363 - val_acc: 0.8025
Epoch 34/40
 - 67s - loss: 0.0483 - acc: 0.9873 - val_loss: 1.2468 - val_acc: 0.8197
Epoch 35/40
 - 67s - loss: 0.0577 - acc: 0.9833 - val_loss: 1.1921 - val_acc: 0.8215
Epoch 36/40
 - 67s - loss: 0.0438 - acc: 0.9874 - val_loss: 1.2003 - val_acc: 0.8281
Epoch 37/40
 - 67s - loss: 0.0433 - acc: 0.9876 - val_loss: 1.2197 - val_acc: 0.8224
Epoch 38/40
 - 67s - loss: 0.0464 - acc: 0.9876 - val_loss: 1.1680 - val_acc: 0.8255
Epoch 39/40
 - 67s - loss: 0.0449 - acc: 0.9876 - val_loss: 1.1578 - val_acc: 0.8343
Epoch 40/40
 - 67s - loss: 0.0429 - acc: 0.9884 - val_loss: 1.2542 - val_acc: 0.8263
Micro-average quality numbers
Precision: 0.7272, Recall: 0.7272, F1-measure: 0.7272
Macro-average quality numbers
Precision: 0.7255, Recall: 0.7205, F1-measure: 0.7193
All-Class quality numbers
Precision: 
[ 0.58536585  0.63209877  0.73898305  0.61696658  0.66815145  0.70646766
  0.61333333  0.77624309  0.81333333  0.84473684  0.90096618  0.9093199
  0.59411765  0.81707317  0.80541872  0.79746835  0.63656388  0.95517241
  0.54754098  0.54976303], 
Recall: 
[ 0.7523511   0.65809769  0.55329949  0.6122449   0.77922078  0.71898734
  0.70769231  0.70959596  0.91959799  0.80856423  0.93483709  0.91161616
  0.51399491  0.67676768  0.82994924  0.79145729  0.79395604  0.73670213
  0.53870968  0.46215139], 
F1-measure: 
[ 0.65843621  0.64483627  0.63280116  0.61459667  0.71942446  0.71267252
  0.65714286  0.7414248   0.86320755  0.82625483  0.91758918  0.91046658
  0.55115962  0.74033149  0.8175      0.79445145  0.70660147  0.83183183
  0.54308943  0.5021645 ]
  
  
  
  
  
  
  
KerasBlog_CNN_Classifier
filter_sizes=[5,5,5]
filter_counts=[200,200,200]
learning_rate=0.001
batch_size=128
num_epochs=50
Words=5000

RESULTS:
Total Doc Count :  18846
Label dimention :  (11314, 20)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  18846
Num of words per Doc :  12013
Number of unique Words :  4998
Words not found in embeddings :  2109
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4998, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5, 5],  							
Filter-Counts : [200, 200, 200], 						
Pool-Windows : [5, 5, 5]
Nm of classes :  20
Input tensor shape:  (None, 1000)
2018-03-29 10:32:41.297193: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-29 10:32:41.297241: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-29 10:32:41.297267: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 1000, 300)
Convolution shape at loop  0  :  (None, 996, 200)
Max-Pool shape at loop  0  :  (None, 199, 200)
Convolution shape at loop  1  :  (None, 195, 200)
Max-Pool shape at loop  1  :  (None, 39, 200)
Convolution shape at loop  2  :  (None, 35, 200)
Max-Pool shape at loop  2  :  (None, 7, 200)
Train on 10182 samples, validate on 1132 samples
Epoch 1/50
 - 145s - loss: 2.9402 - acc: 0.0767 - val_loss: 2.6927 - val_acc: 0.1078
Epoch 2/50
 - 169s - loss: 2.4123 - acc: 0.1768 - val_loss: 1.8301 - val_acc: 0.3675
Epoch 3/50
 - 168s - loss: 1.6351 - acc: 0.4091 - val_loss: 1.3621 - val_acc: 0.5124
Epoch 4/50
 - 169s - loss: 1.1473 - acc: 0.5709 - val_loss: 1.0505 - val_acc: 0.6519
Epoch 5/50
 - 168s - loss: 0.8336 - acc: 0.6959 - val_loss: 0.8952 - val_acc: 0.7120
Epoch 6/50
 - 169s - loss: 0.6367 - acc: 0.7812 - val_loss: 0.8516 - val_acc: 0.7438
Epoch 7/50
 - 168s - loss: 0.4830 - acc: 0.8422 - val_loss: 0.7320 - val_acc: 0.7906
Epoch 8/50
 - 168s - loss: 0.3681 - acc: 0.8838 - val_loss: 0.7047 - val_acc: 0.8004
Epoch 9/50
 - 169s - loss: 0.2949 - acc: 0.9068 - val_loss: 0.8207 - val_acc: 0.7906
Epoch 10/50
 - 126s - loss: 0.2140 - acc: 0.9340 - val_loss: 0.7574 - val_acc: 0.8136
Epoch 11/50
 - 116s - loss: 0.1930 - acc: 0.9446 - val_loss: 0.7490 - val_acc: 0.8136
Epoch 12/50
 - 116s - loss: 0.1642 - acc: 0.9519 - val_loss: 0.8089 - val_acc: 0.8189
Epoch 13/50
 - 116s - loss: 0.1275 - acc: 0.9624 - val_loss: 0.8376 - val_acc: 0.8189
Epoch 14/50
 - 116s - loss: 0.1115 - acc: 0.9663 - val_loss: 0.8615 - val_acc: 0.8198
Epoch 15/50
 - 116s - loss: 0.1063 - acc: 0.9707 - val_loss: 0.9132 - val_acc: 0.8180
Epoch 16/50
 - 116s - loss: 0.1035 - acc: 0.9718 - val_loss: 0.9968 - val_acc: 0.8163
Epoch 17/50
 - 116s - loss: 0.0896 - acc: 0.9758 - val_loss: 0.9552 - val_acc: 0.8286
Epoch 18/50
 - 116s - loss: 0.0659 - acc: 0.9804 - val_loss: 0.9099 - val_acc: 0.8366
Epoch 19/50
 - 116s - loss: 0.0641 - acc: 0.9830 - val_loss: 0.9170 - val_acc: 0.8428
Epoch 20/50
 - 116s - loss: 0.0601 - acc: 0.9843 - val_loss: 0.9291 - val_acc: 0.8339
Epoch 21/50
 - 116s - loss: 0.0686 - acc: 0.9809 - val_loss: 1.0119 - val_acc: 0.8313
Epoch 22/50
 - 116s - loss: 0.0674 - acc: 0.9815 - val_loss: 0.8976 - val_acc: 0.8481
Epoch 23/50
 - 116s - loss: 0.0427 - acc: 0.9891 - val_loss: 0.9324 - val_acc: 0.8534
Epoch 24/50
 - 116s - loss: 0.0506 - acc: 0.9865 - val_loss: 1.0090 - val_acc: 0.8419
Epoch 25/50
 - 116s - loss: 0.0511 - acc: 0.9848 - val_loss: 0.9549 - val_acc: 0.8419
Epoch 26/50
 - 116s - loss: 0.0406 - acc: 0.9896 - val_loss: 1.0545 - val_acc: 0.8436
Epoch 27/50
 - 116s - loss: 0.0397 - acc: 0.9897 - val_loss: 1.0217 - val_acc: 0.8481
Epoch 28/50
 - 116s - loss: 0.0491 - acc: 0.9863 - val_loss: 0.9527 - val_acc: 0.8419
Epoch 29/50
 - 116s - loss: 0.0383 - acc: 0.9904 - val_loss: 0.9857 - val_acc: 0.8489
Epoch 30/50
 - 116s - loss: 0.0541 - acc: 0.9850 - val_loss: 1.2442 - val_acc: 0.8101
Epoch 31/50
 - 116s - loss: 0.0520 - acc: 0.9855 - val_loss: 1.0740 - val_acc: 0.8357
Epoch 32/50
 - 116s - loss: 0.0496 - acc: 0.9870 - val_loss: 1.0679 - val_acc: 0.8366
Epoch 33/50
 - 116s - loss: 0.0420 - acc: 0.9891 - val_loss: 1.1593 - val_acc: 0.8154
Epoch 34/50
 - 116s - loss: 0.0499 - acc: 0.9855 - val_loss: 1.0361 - val_acc: 0.8366
Epoch 35/50
 - 116s - loss: 0.0413 - acc: 0.9896 - val_loss: 0.9961 - val_acc: 0.8489
Epoch 36/50
 - 116s - loss: 0.0287 - acc: 0.9930 - val_loss: 1.0492 - val_acc: 0.8410
Epoch 37/50
 - 116s - loss: 0.0318 - acc: 0.9914 - val_loss: 1.0191 - val_acc: 0.8516
Epoch 38/50
 - 116s - loss: 0.0287 - acc: 0.9929 - val_loss: 0.9769 - val_acc: 0.8542
Epoch 39/50
 - 116s - loss: 0.0256 - acc: 0.9927 - val_loss: 1.0547 - val_acc: 0.8410
Epoch 40/50
 - 116s - loss: 0.0245 - acc: 0.9939 - val_loss: 1.2507 - val_acc: 0.8260
Epoch 41/50
 - 116s - loss: 0.0460 - acc: 0.9888 - val_loss: 1.0886 - val_acc: 0.8392
Epoch 42/50
 - 117s - loss: 0.0461 - acc: 0.9881 - val_loss: 0.9850 - val_acc: 0.8595
Epoch 43/50
 - 116s - loss: 0.0360 - acc: 0.9906 - val_loss: 0.9863 - val_acc: 0.8436
Epoch 44/50
 - 116s - loss: 0.0345 - acc: 0.9919 - val_loss: 1.1170 - val_acc: 0.8489
Epoch 45/50
 - 116s - loss: 0.0366 - acc: 0.9910 - val_loss: 0.9899 - val_acc: 0.8534
Epoch 46/50
 - 116s - loss: 0.0255 - acc: 0.9933 - val_loss: 1.1046 - val_acc: 0.8428
Epoch 47/50
 - 116s - loss: 0.0286 - acc: 0.9916 - val_loss: 1.0595 - val_acc: 0.8498
Epoch 48/50
 - 116s - loss: 0.0322 - acc: 0.9919 - val_loss: 1.2487 - val_acc: 0.8330
Epoch 49/50
 - 115s - loss: 0.0349 - acc: 0.9906 - val_loss: 1.1454 - val_acc: 0.8481
Epoch 50/50
 - 117s - loss: 0.0166 - acc: 0.9956 - val_loss: 1.0347 - val_acc: 0.8551
Micro-average quality numbers
Precision: 0.7492, Recall: 0.7492, F1-measure: 0.7492
Macro-average quality numbers
Precision: 0.7477, Recall: 0.7429, F1-measure: 0.7434
All-Class quality numbers
Precision: 
[ 0.71692308  0.71348315  0.6503856   0.57238307  0.65426696  0.79705882
  0.61894737  0.75682382  0.90591398  0.95428571  0.94162437  0.88366337
  0.63663664  0.82        0.84102564  0.79086538  0.69230769  0.8852459
  0.59859155  0.524     ], 
Recall: 
[ 0.73040752  0.6529563   0.64213198  0.65561224  0.77662338  0.68607595
  0.75384615  0.77020202  0.84673367  0.84130982  0.92982456  0.90151515
  0.5394402   0.72474747  0.83248731  0.82663317  0.81593407  0.86170213
  0.5483871   0.52191235], 
F1-measure: 
[ 0.72360248  0.68187919  0.64623244  0.61117717  0.71021378  0.73741497
  0.67976879  0.76345432  0.87532468  0.89424364  0.93568726  0.8925
  0.58402204  0.769437    0.83673469  0.80835381  0.74905422  0.87331536
  0.57239057  0.52295409]




#########################################################################################
Reuters 21578
KerasBlog_CNN_Classifier
filter_sizes=[5,5]
filter_counts=[128,128]
learning_rate=0.001
batch_size=64
num_epochs=30

RESULTS:
Label dimention :  (5735, 10)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  7980
Num of words per Doc :  595
Number of unique Words :  10910
Words not found in embeddings :  3072
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (10910, 300)
Using Nested CNN with parameters : 
Batch-size : 64,  								
Filter-Sizes : [5, 5],  							
Filter-Counts : [128, 128], 							
Pool-Windows : [2, 2]
Nm of classes :  10
Input tensor shape:  (None, 595)
2018-03-22 19:54:05.192448: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-22 19:54:05.192508: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-22 19:54:05.192524: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 595, 300)
Convolution shape at loop  0  :  (None, 591, 128)
Max-Pool shape at loop  0  :  (None, 118, 128)
Convolution shape at loop  1  :  (None, 114, 128)
Max-Pool shape at loop  1  :  (None, 22, 128)
Train on 4588 samples, validate on 1147 samples
Epoch 1/30
 - 34s - loss: 0.8400 - acc: 0.6626 - val_loss: 9.8244 - val_acc: 0.1473
Epoch 2/30
 - 31s - loss: 0.3364 - acc: 0.8895 - val_loss: 8.0469 - val_acc: 0.1465
Epoch 3/30
 - 31s - loss: 0.1541 - acc: 0.9486 - val_loss: 10.6487 - val_acc: 0.1482
Epoch 4/30
 - 30s - loss: 0.0744 - acc: 0.9754 - val_loss: 10.3274 - val_acc: 0.1465
Epoch 5/30
 - 31s - loss: 0.0337 - acc: 0.9911 - val_loss: 11.1619 - val_acc: 0.1482
Epoch 6/30
 - 30s - loss: 0.0145 - acc: 0.9972 - val_loss: 11.4847 - val_acc: 0.1465
Epoch 7/30
 - 31s - loss: 0.0085 - acc: 0.9987 - val_loss: 11.8424 - val_acc: 0.1482
Epoch 8/30
 - 30s - loss: 0.0076 - acc: 0.9985 - val_loss: 12.5864 - val_acc: 0.1465
Epoch 9/30
 - 31s - loss: 0.0038 - acc: 0.9993 - val_loss: 13.0881 - val_acc: 0.1465
Epoch 10/30
 - 31s - loss: 0.0050 - acc: 0.9993 - val_loss: 11.8543 - val_acc: 0.1482
Epoch 11/30
 - 31s - loss: 0.0030 - acc: 0.9998 - val_loss: 12.8291 - val_acc: 0.1473
Epoch 12/30
 - 30s - loss: 0.0013 - acc: 0.9998 - val_loss: 12.7947 - val_acc: 0.1482
Epoch 13/30
 - 30s - loss: 8.4326e-04 - acc: 1.0000 - val_loss: 12.5858 - val_acc: 0.1465
Epoch 14/30
 - 31s - loss: 8.0220e-04 - acc: 1.0000 - val_loss: 13.1685 - val_acc: 0.1465
Epoch 15/30
 - 30s - loss: 0.0161 - acc: 0.9950 - val_loss: 11.3591 - val_acc: 0.1473
Epoch 16/30
 - 30s - loss: 0.0048 - acc: 0.9987 - val_loss: 11.8425 - val_acc: 0.1473
Epoch 17/30
 - 30s - loss: 7.9652e-04 - acc: 1.0000 - val_loss: 12.3015 - val_acc: 0.1473
Epoch 18/30
 - 30s - loss: 6.0245e-04 - acc: 0.9998 - val_loss: 12.5215 - val_acc: 0.1473
Epoch 19/30
 - 30s - loss: 3.5156e-04 - acc: 1.0000 - val_loss: 12.6229 - val_acc: 0.1473
Epoch 20/30
 - 35s - loss: 4.8616e-04 - acc: 1.0000 - val_loss: 12.7173 - val_acc: 0.1473
Epoch 21/30
 - 31s - loss: 0.0082 - acc: 0.9976 - val_loss: 11.9034 - val_acc: 0.1465
Epoch 22/30
 - 31s - loss: 0.0018 - acc: 0.9991 - val_loss: 12.1998 - val_acc: 0.1473
Epoch 23/30
 - 30s - loss: 3.6857e-04 - acc: 1.0000 - val_loss: 12.0069 - val_acc: 0.1473
Epoch 24/30
 - 30s - loss: 1.7942e-04 - acc: 1.0000 - val_loss: 12.7313 - val_acc: 0.1473
Epoch 25/30
 - 30s - loss: 2.0002e-04 - acc: 1.0000 - val_loss: 12.6729 - val_acc: 0.1473
Epoch 26/30
 - 30s - loss: 1.6641e-04 - acc: 1.0000 - val_loss: 12.8942 - val_acc: 0.1473
Epoch 27/30
 - 30s - loss: 1.0854e-04 - acc: 1.0000 - val_loss: 12.7713 - val_acc: 0.1473
Epoch 28/30
 - 31s - loss: 0.0186 - acc: 0.9961 - val_loss: 11.4686 - val_acc: 0.1421
Epoch 29/30
 - 30s - loss: 0.0133 - acc: 0.9965 - val_loss: 10.6894 - val_acc: 0.1456
Epoch 30/30
 - 31s - loss: 0.0029 - acc: 0.9991 - val_loss: 11.1964 - val_acc: 0.1456

