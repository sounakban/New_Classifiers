################################################################################
20 NewsGroups:

KerasBlog_CNN_Classifier
filter_sizes=[5,5,5]
filter_counts=[128,128,128]
learning_rate=0.001
batch_size=128
num_epochs=40
Words=5000

RESULTS:
Total Doc Count :  18846
Label dimention :  (11314, 20)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  18846
Num of words per Doc :  12013
Number of unique Words :  4998
Words not found in embeddings :  2109
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4998, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5, 5],  							
Filter-Counts : [128, 128, 128], 						
Pool-Windows : [5, 5, 5]
Nm of classes :  20
Input tensor shape:  (None, 1000)
2018-03-28 21:12:06.495334: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-28 21:12:06.495404: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-28 21:12:06.495452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 1000, 300)
Convolution shape at loop  0  :  (None, 996, 128)
Max-Pool shape at loop  0  :  (None, 199, 128)
Convolution shape at loop  1  :  (None, 195, 128)
Max-Pool shape at loop  1  :  (None, 39, 128)
Convolution shape at loop  2  :  (None, 35, 128)
Max-Pool shape at loop  2  :  (None, 7, 128)
Train on 9051 samples, validate on 2263 samples
Epoch 1/40
 - 67s - loss: 2.9728 - acc: 0.0679 - val_loss: 2.8762 - val_acc: 0.0884
Epoch 2/40
 - 67s - loss: 2.6484 - acc: 0.1248 - val_loss: 2.2032 - val_acc: 0.2762
Epoch 3/40
 - 67s - loss: 1.9970 - acc: 0.2845 - val_loss: 1.5584 - val_acc: 0.4264
Epoch 4/40
 - 67s - loss: 1.4886 - acc: 0.4320 - val_loss: 1.3005 - val_acc: 0.5320
Epoch 5/40
 - 67s - loss: 1.1982 - acc: 0.5555 - val_loss: 1.1320 - val_acc: 0.5868
Epoch 6/40
 - 67s - loss: 0.9782 - acc: 0.6256 - val_loss: 1.0264 - val_acc: 0.6381
Epoch 7/40
 - 67s - loss: 0.8537 - acc: 0.6806 - val_loss: 0.9789 - val_acc: 0.6920
Epoch 8/40
 - 67s - loss: 0.7130 - acc: 0.7390 - val_loss: 1.0219 - val_acc: 0.6982
Epoch 9/40
 - 67s - loss: 0.6073 - acc: 0.7871 - val_loss: 1.0187 - val_acc: 0.7212
Epoch 10/40
 - 67s - loss: 0.4836 - acc: 0.8373 - val_loss: 0.8684 - val_acc: 0.7627
Epoch 11/40
 - 66s - loss: 0.4065 - acc: 0.8673 - val_loss: 0.8147 - val_acc: 0.7941
Epoch 12/40
 - 67s - loss: 0.3469 - acc: 0.8895 - val_loss: 0.8308 - val_acc: 0.7817
Epoch 13/40
 - 66s - loss: 0.2907 - acc: 0.9047 - val_loss: 0.8435 - val_acc: 0.7998
Epoch 14/40
 - 67s - loss: 0.2413 - acc: 0.9241 - val_loss: 0.8425 - val_acc: 0.8118
Epoch 15/40
 - 67s - loss: 0.2097 - acc: 0.9360 - val_loss: 0.8859 - val_acc: 0.8047
Epoch 16/40
 - 67s - loss: 0.1864 - acc: 0.9453 - val_loss: 0.8780 - val_acc: 0.8060
Epoch 17/40
 - 67s - loss: 0.1535 - acc: 0.9514 - val_loss: 1.0010 - val_acc: 0.7976
Epoch 18/40
 - 67s - loss: 0.1496 - acc: 0.9576 - val_loss: 0.9715 - val_acc: 0.8016
Epoch 19/40
 - 67s - loss: 0.1413 - acc: 0.9582 - val_loss: 1.0071 - val_acc: 0.8175
Epoch 20/40
 - 67s - loss: 0.1384 - acc: 0.9580 - val_loss: 0.9922 - val_acc: 0.8051
Epoch 21/40
 - 67s - loss: 0.1483 - acc: 0.9540 - val_loss: 0.9122 - val_acc: 0.8237
Epoch 22/40
 - 67s - loss: 0.0968 - acc: 0.9737 - val_loss: 1.0006 - val_acc: 0.8171
Epoch 23/40
 - 67s - loss: 0.0995 - acc: 0.9753 - val_loss: 1.0042 - val_acc: 0.8277
Epoch 24/40
 - 67s - loss: 0.0843 - acc: 0.9772 - val_loss: 1.0654 - val_acc: 0.8144
Epoch 25/40
 - 67s - loss: 0.0881 - acc: 0.9745 - val_loss: 1.1007 - val_acc: 0.8188
Epoch 26/40
 - 67s - loss: 0.0759 - acc: 0.9790 - val_loss: 1.1218 - val_acc: 0.8162
Epoch 27/40
 - 67s - loss: 0.0696 - acc: 0.9793 - val_loss: 1.2956 - val_acc: 0.8003
Epoch 28/40
 - 68s - loss: 0.0724 - acc: 0.9779 - val_loss: 1.2382 - val_acc: 0.8091
Epoch 29/40
 - 67s - loss: 0.0641 - acc: 0.9821 - val_loss: 1.1315 - val_acc: 0.8312
Epoch 30/40
 - 67s - loss: 0.0790 - acc: 0.9774 - val_loss: 1.0880 - val_acc: 0.8263
Epoch 31/40
 - 67s - loss: 0.0517 - acc: 0.9854 - val_loss: 1.1405 - val_acc: 0.8281
Epoch 32/40
 - 67s - loss: 0.0645 - acc: 0.9815 - val_loss: 1.2242 - val_acc: 0.8219
Epoch 33/40
 - 67s - loss: 0.0539 - acc: 0.9849 - val_loss: 1.3363 - val_acc: 0.8025
Epoch 34/40
 - 67s - loss: 0.0483 - acc: 0.9873 - val_loss: 1.2468 - val_acc: 0.8197
Epoch 35/40
 - 67s - loss: 0.0577 - acc: 0.9833 - val_loss: 1.1921 - val_acc: 0.8215
Epoch 36/40
 - 67s - loss: 0.0438 - acc: 0.9874 - val_loss: 1.2003 - val_acc: 0.8281
Epoch 37/40
 - 67s - loss: 0.0433 - acc: 0.9876 - val_loss: 1.2197 - val_acc: 0.8224
Epoch 38/40
 - 67s - loss: 0.0464 - acc: 0.9876 - val_loss: 1.1680 - val_acc: 0.8255
Epoch 39/40
 - 67s - loss: 0.0449 - acc: 0.9876 - val_loss: 1.1578 - val_acc: 0.8343
Epoch 40/40
 - 67s - loss: 0.0429 - acc: 0.9884 - val_loss: 1.2542 - val_acc: 0.8263
Micro-average quality numbers
Precision: 0.7272, Recall: 0.7272, F1-measure: 0.7272
Macro-average quality numbers
Precision: 0.7255, Recall: 0.7205, F1-measure: 0.7193
All-Class quality numbers
Precision: 
[ 0.58536585  0.63209877  0.73898305  0.61696658  0.66815145  0.70646766
  0.61333333  0.77624309  0.81333333  0.84473684  0.90096618  0.9093199
  0.59411765  0.81707317  0.80541872  0.79746835  0.63656388  0.95517241
  0.54754098  0.54976303], 
Recall: 
[ 0.7523511   0.65809769  0.55329949  0.6122449   0.77922078  0.71898734
  0.70769231  0.70959596  0.91959799  0.80856423  0.93483709  0.91161616
  0.51399491  0.67676768  0.82994924  0.79145729  0.79395604  0.73670213
  0.53870968  0.46215139], 
F1-measure: 
[ 0.65843621  0.64483627  0.63280116  0.61459667  0.71942446  0.71267252
  0.65714286  0.7414248   0.86320755  0.82625483  0.91758918  0.91046658
  0.55115962  0.74033149  0.8175      0.79445145  0.70660147  0.83183183
  0.54308943  0.5021645 ]
  
  
  
  
  
  
  
KerasBlog_CNN_Classifier
filter_sizes=[5,5,5]
filter_counts=[200,200,200]
learning_rate=0.001
batch_size=128
num_epochs=50
Words=5000

RESULTS:
Total Doc Count :  18846
Label dimention :  (11314, 20)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  18846
Num of words per Doc :  12013
Number of unique Words :  4998
Words not found in embeddings :  2109
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4998, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5, 5],  							
Filter-Counts : [200, 200, 200], 						
Pool-Windows : [5, 5, 5]
Nm of classes :  20
Input tensor shape:  (None, 1000)
2018-03-29 10:32:41.297193: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-29 10:32:41.297241: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-29 10:32:41.297267: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 1000, 300)
Convolution shape at loop  0  :  (None, 996, 200)
Max-Pool shape at loop  0  :  (None, 199, 200)
Convolution shape at loop  1  :  (None, 195, 200)
Max-Pool shape at loop  1  :  (None, 39, 200)
Convolution shape at loop  2  :  (None, 35, 200)
Max-Pool shape at loop  2  :  (None, 7, 200)
Train on 10182 samples, validate on 1132 samples
Epoch 1/50
 - 145s - loss: 2.9402 - acc: 0.0767 - val_loss: 2.6927 - val_acc: 0.1078
Epoch 2/50
 - 169s - loss: 2.4123 - acc: 0.1768 - val_loss: 1.8301 - val_acc: 0.3675
Epoch 3/50
 - 168s - loss: 1.6351 - acc: 0.4091 - val_loss: 1.3621 - val_acc: 0.5124
Epoch 4/50
 - 169s - loss: 1.1473 - acc: 0.5709 - val_loss: 1.0505 - val_acc: 0.6519
Epoch 5/50
 - 168s - loss: 0.8336 - acc: 0.6959 - val_loss: 0.8952 - val_acc: 0.7120
Epoch 6/50
 - 169s - loss: 0.6367 - acc: 0.7812 - val_loss: 0.8516 - val_acc: 0.7438
Epoch 7/50
 - 168s - loss: 0.4830 - acc: 0.8422 - val_loss: 0.7320 - val_acc: 0.7906
Epoch 8/50
 - 168s - loss: 0.3681 - acc: 0.8838 - val_loss: 0.7047 - val_acc: 0.8004
Epoch 9/50
 - 169s - loss: 0.2949 - acc: 0.9068 - val_loss: 0.8207 - val_acc: 0.7906
Epoch 10/50
 - 126s - loss: 0.2140 - acc: 0.9340 - val_loss: 0.7574 - val_acc: 0.8136
Epoch 11/50
 - 116s - loss: 0.1930 - acc: 0.9446 - val_loss: 0.7490 - val_acc: 0.8136
Epoch 12/50
 - 116s - loss: 0.1642 - acc: 0.9519 - val_loss: 0.8089 - val_acc: 0.8189
Epoch 13/50
 - 116s - loss: 0.1275 - acc: 0.9624 - val_loss: 0.8376 - val_acc: 0.8189
Epoch 14/50
 - 116s - loss: 0.1115 - acc: 0.9663 - val_loss: 0.8615 - val_acc: 0.8198
Epoch 15/50
 - 116s - loss: 0.1063 - acc: 0.9707 - val_loss: 0.9132 - val_acc: 0.8180
Epoch 16/50
 - 116s - loss: 0.1035 - acc: 0.9718 - val_loss: 0.9968 - val_acc: 0.8163
Epoch 17/50
 - 116s - loss: 0.0896 - acc: 0.9758 - val_loss: 0.9552 - val_acc: 0.8286
Epoch 18/50
 - 116s - loss: 0.0659 - acc: 0.9804 - val_loss: 0.9099 - val_acc: 0.8366
Epoch 19/50
 - 116s - loss: 0.0641 - acc: 0.9830 - val_loss: 0.9170 - val_acc: 0.8428
Epoch 20/50
 - 116s - loss: 0.0601 - acc: 0.9843 - val_loss: 0.9291 - val_acc: 0.8339
Epoch 21/50
 - 116s - loss: 0.0686 - acc: 0.9809 - val_loss: 1.0119 - val_acc: 0.8313
Epoch 22/50
 - 116s - loss: 0.0674 - acc: 0.9815 - val_loss: 0.8976 - val_acc: 0.8481
Epoch 23/50
 - 116s - loss: 0.0427 - acc: 0.9891 - val_loss: 0.9324 - val_acc: 0.8534
Epoch 24/50
 - 116s - loss: 0.0506 - acc: 0.9865 - val_loss: 1.0090 - val_acc: 0.8419
Epoch 25/50
 - 116s - loss: 0.0511 - acc: 0.9848 - val_loss: 0.9549 - val_acc: 0.8419
Epoch 26/50
 - 116s - loss: 0.0406 - acc: 0.9896 - val_loss: 1.0545 - val_acc: 0.8436
Epoch 27/50
 - 116s - loss: 0.0397 - acc: 0.9897 - val_loss: 1.0217 - val_acc: 0.8481
Epoch 28/50
 - 116s - loss: 0.0491 - acc: 0.9863 - val_loss: 0.9527 - val_acc: 0.8419
Epoch 29/50
 - 116s - loss: 0.0383 - acc: 0.9904 - val_loss: 0.9857 - val_acc: 0.8489
Epoch 30/50
 - 116s - loss: 0.0541 - acc: 0.9850 - val_loss: 1.2442 - val_acc: 0.8101
Epoch 31/50
 - 116s - loss: 0.0520 - acc: 0.9855 - val_loss: 1.0740 - val_acc: 0.8357
Epoch 32/50
 - 116s - loss: 0.0496 - acc: 0.9870 - val_loss: 1.0679 - val_acc: 0.8366
Epoch 33/50
 - 116s - loss: 0.0420 - acc: 0.9891 - val_loss: 1.1593 - val_acc: 0.8154
Epoch 34/50
 - 116s - loss: 0.0499 - acc: 0.9855 - val_loss: 1.0361 - val_acc: 0.8366
Epoch 35/50
 - 116s - loss: 0.0413 - acc: 0.9896 - val_loss: 0.9961 - val_acc: 0.8489
Epoch 36/50
 - 116s - loss: 0.0287 - acc: 0.9930 - val_loss: 1.0492 - val_acc: 0.8410
Epoch 37/50
 - 116s - loss: 0.0318 - acc: 0.9914 - val_loss: 1.0191 - val_acc: 0.8516
Epoch 38/50
 - 116s - loss: 0.0287 - acc: 0.9929 - val_loss: 0.9769 - val_acc: 0.8542
Epoch 39/50
 - 116s - loss: 0.0256 - acc: 0.9927 - val_loss: 1.0547 - val_acc: 0.8410
Epoch 40/50
 - 116s - loss: 0.0245 - acc: 0.9939 - val_loss: 1.2507 - val_acc: 0.8260
Epoch 41/50
 - 116s - loss: 0.0460 - acc: 0.9888 - val_loss: 1.0886 - val_acc: 0.8392
Epoch 42/50
 - 117s - loss: 0.0461 - acc: 0.9881 - val_loss: 0.9850 - val_acc: 0.8595
Epoch 43/50
 - 116s - loss: 0.0360 - acc: 0.9906 - val_loss: 0.9863 - val_acc: 0.8436
Epoch 44/50
 - 116s - loss: 0.0345 - acc: 0.9919 - val_loss: 1.1170 - val_acc: 0.8489
Epoch 45/50
 - 116s - loss: 0.0366 - acc: 0.9910 - val_loss: 0.9899 - val_acc: 0.8534
Epoch 46/50
 - 116s - loss: 0.0255 - acc: 0.9933 - val_loss: 1.1046 - val_acc: 0.8428
Epoch 47/50
 - 116s - loss: 0.0286 - acc: 0.9916 - val_loss: 1.0595 - val_acc: 0.8498
Epoch 48/50
 - 116s - loss: 0.0322 - acc: 0.9919 - val_loss: 1.2487 - val_acc: 0.8330
Epoch 49/50
 - 115s - loss: 0.0349 - acc: 0.9906 - val_loss: 1.1454 - val_acc: 0.8481
Epoch 50/50
 - 117s - loss: 0.0166 - acc: 0.9956 - val_loss: 1.0347 - val_acc: 0.8551
Micro-average quality numbers
Precision: 0.7492, Recall: 0.7492, F1-measure: 0.7492
Macro-average quality numbers
Precision: 0.7477, Recall: 0.7429, F1-measure: 0.7434
All-Class quality numbers
Precision: 
[ 0.71692308  0.71348315  0.6503856   0.57238307  0.65426696  0.79705882
  0.61894737  0.75682382  0.90591398  0.95428571  0.94162437  0.88366337
  0.63663664  0.82        0.84102564  0.79086538  0.69230769  0.8852459
  0.59859155  0.524     ], 
Recall: 
[ 0.73040752  0.6529563   0.64213198  0.65561224  0.77662338  0.68607595
  0.75384615  0.77020202  0.84673367  0.84130982  0.92982456  0.90151515
  0.5394402   0.72474747  0.83248731  0.82663317  0.81593407  0.86170213
  0.5483871   0.52191235], 
F1-measure: 
[ 0.72360248  0.68187919  0.64623244  0.61117717  0.71021378  0.73741497
  0.67976879  0.76345432  0.87532468  0.89424364  0.93568726  0.8925
  0.58402204  0.769437    0.83673469  0.80835381  0.74905422  0.87331536
  0.57239057  0.52295409]




#########################################################################################
Reuters 21578

## Multiclass ##
KerasBlog_CNN_Classifier
filter_sizes=[5,5]
filter_counts=[200,200]
learning_rate=0.001
batch_size=64
num_epochs=10
Dropout=0.8
l1(0.01) & l2(0.01) regularization

RESULTS:
Label dimention :  (5735, 10)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  7980
Num of words per Doc :  262
Number of unique Words :  501
Words not found in embeddings :  69
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (501, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5],  							
Filter-Counts : [200, 200], 							
Learning Rate : 0.001, 								
Pool-Windows : [2, 2]
Nm of classes :  10
Input tensor shape:  (None, 262)
2018-04-04 12:26:16.122653: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-04 12:26:16.122738: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-04 12:26:16.122766: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 262, 300)
Convolution shape at loop  0  :  (None, 258, 200)
Max-Pool shape at loop  0  :  (None, 51, 200)
Convolution shape at loop  1  :  (None, 47, 200)
Max-Pool shape at loop  1  :  (None, 9, 200)
Train on 5735 samples, validate on 2245 samples
Epoch 1/15
 - 17s - loss: 2.8819 - acc: 0.5226 - val_loss: 2.2022 - val_acc: 0.7751
Epoch 2/15
 - 16s - loss: 2.1802 - acc: 0.7519 - val_loss: 1.9009 - val_acc: 0.8347
Epoch 3/15
 - 17s - loss: 1.9580 - acc: 0.8099 - val_loss: 1.7355 - val_acc: 0.8820
Epoch 4/15
 - 17s - loss: 1.8130 - acc: 0.8521 - val_loss: 1.6366 - val_acc: 0.9105
Epoch 5/15
 - 17s - loss: 1.6907 - acc: 0.8884 - val_loss: 1.5582 - val_acc: 0.9269
Epoch 6/15
 - 16s - loss: 1.6148 - acc: 0.9137 - val_loss: 1.5268 - val_acc: 0.9394
Epoch 7/15
 - 17s - loss: 1.5551 - acc: 0.9289 - val_loss: 1.5162 - val_acc: 0.9448
Epoch 8/15
 - 16s - loss: 1.5198 - acc: 0.9458 - val_loss: 1.4793 - val_acc: 0.9510
Epoch 9/15
 - 17s - loss: 1.4875 - acc: 0.9522 - val_loss: 1.4701 - val_acc: 0.9590
Epoch 10/15
 - 17s - loss: 1.4607 - acc: 0.9594 - val_loss: 1.4587 - val_acc: 0.9612
Epoch 11/15
 - 19s - loss: 1.4441 - acc: 0.9622 - val_loss: 1.4722 - val_acc: 0.9563
Epoch 12/15
 - 20s - loss: 1.4221 - acc: 0.9705 - val_loss: 1.4498 - val_acc: 0.9653
Epoch 13/15
 - 17s - loss: 1.4170 - acc: 0.9700 - val_loss: 1.4711 - val_acc: 0.9604
Epoch 14/15
 - 16s - loss: 1.4052 - acc: 0.9726 - val_loss: 1.4541 - val_acc: 0.9635
Epoch 15/15
 - 16s - loss: 1.3872 - acc: 0.9801 - val_loss: 1.4493 - val_acc: 0.9657
Shape of 
1.test_labels :  (2245, 10) 
2.predictions :  (2245, 10)
Micro-average quality numbers
Precision: 0.9657, Recall: 0.9657, F1-measure: 0.9657
Macro-average quality numbers
Precision: 0.9204, Recall: 0.9265, F1-measure: 0.9232
All-Class quality numbers
Precision: 
[ 0.98115942  0.95454545  0.95689655  0.98795181  0.83544304  0.81395349
  0.92857143  0.89473684  0.96        0.8902439 ], 
Recall: 
[ 0.97410072  0.95454545  0.93277311  0.99070632  0.81481481  0.8045977
  0.92857143  0.94444444  0.96        0.96052632], 
F1-measure: 
[ 0.97761733  0.95454545  0.94468085  0.98932715  0.825       0.80924855
  0.92857143  0.91891892  0.96        0.92405063]
  
  
  
  
  
  
  
  
## Multi-label ##
Label dimention :  (7722, 90)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  10727
Num of words per Doc :  312
Number of unique Words :  501
Words not found in embeddings :  82
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (501, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5],  							
Filter-Counts : [300, 300], 							
Learning Rate : 0.001, 								
Pool-Windows : [2, 2]
Nm of classes :  90
Input tensor shape:  (None, 312)
2018-04-05 15:56:42.658726: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-05 15:56:42.658801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-05 15:56:42.658818: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 312, 300)
Convolution shape at loop  0  :  (None, 308, 300)
Max-Pool shape at loop  0  :  (None, 61, 300)
Convolution shape at loop  1  :  (None, 57, 300)
Max-Pool shape at loop  1  :  (None, 11, 300)
Train on 7722 samples, validate on 3005 samples
Epoch 1/17
 - 46s - loss: 0.0866 - acc: 0.9721 - val_loss: 0.0410 - val_acc: 0.9896
Epoch 2/17
 - 45s - loss: 0.0440 - acc: 0.9893 - val_loss: 0.0380 - val_acc: 0.9913
Epoch 3/17
 - 51s - loss: 0.0399 - acc: 0.9902 - val_loss: 0.0360 - val_acc: 0.9916
Epoch 4/17
 - 45s - loss: 0.0363 - acc: 0.9909 - val_loss: 0.0315 - val_acc: 0.9923
Epoch 5/17
 - 45s - loss: 0.0328 - acc: 0.9916 - val_loss: 0.0289 - val_acc: 0.9929
Epoch 6/17
 - 45s - loss: 0.0314 - acc: 0.9919 - val_loss: 0.0278 - val_acc: 0.9932
Epoch 7/17
 - 45s - loss: 0.0293 - acc: 0.9923 - val_loss: 0.0280 - val_acc: 0.9931
Epoch 8/17
 - 51s - loss: 0.0281 - acc: 0.9925 - val_loss: 0.0273 - val_acc: 0.9933
Epoch 9/17
 - 44s - loss: 0.0272 - acc: 0.9928 - val_loss: 0.0254 - val_acc: 0.9936
Epoch 10/17
 - 45s - loss: 0.0262 - acc: 0.9930 - val_loss: 0.0251 - val_acc: 0.9936
Epoch 11/17
 - 45s - loss: 0.0255 - acc: 0.9931 - val_loss: 0.0247 - val_acc: 0.9938
Epoch 12/17
 - 46s - loss: 0.0248 - acc: 0.9931 - val_loss: 0.0248 - val_acc: 0.9936
Epoch 13/17
 - 51s - loss: 0.0239 - acc: 0.9934 - val_loss: 0.0245 - val_acc: 0.9937
Epoch 14/17
 - 46s - loss: 0.0234 - acc: 0.9935 - val_loss: 0.0234 - val_acc: 0.9941
Epoch 15/17
 - 46s - loss: 0.0227 - acc: 0.9936 - val_loss: 0.0238 - val_acc: 0.9939
Epoch 16/17
 - 46s - loss: 0.0221 - acc: 0.9938 - val_loss: 0.0233 - val_acc: 0.9941
Epoch 17/17
 - 46s - loss: 0.0218 - acc: 0.9938 - val_loss: 0.0229 - val_acc: 0.9941
Type of predictions <class 'numpy.ndarray'>
Shape of 
1.test_labels :  (3005, 90) 
2.predictions :  (3005, 90)
Micro-average quality numbers
Precision: 0.9018, Recall: 0.6453, F1-measure: 0.7523
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
Macro-average quality numbers
Precision: 0.2269, Recall: 0.1106, F1-measure: 0.1306
All-Class quality numbers
Precision: 
[ 0.87119114  0.          0.25        0.625       0.          0.          0.
  0.          0.          0.96        0.          0.          0.74358974
  0.          0.          1.          0.          0.85082873  0.
  0.77777778  0.          0.96792453  0.          0.          0.88235294
  0.          0.96899225  0.          0.          0.          0.          0.
  0.          0.          0.86206897  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.74666667  0.85714286  0.          0.          0.          0.          0.
  0.          0.66666667  0.          0.          1.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.9375      0.          1.
  0.          0.          0.75        0.          0.          0.          0.
  1.          0.          0.          0.81372549  1.          0.89090909
  0.          1.          0.        ], 
Recall: 
[ 0.87604457  0.          0.07142857  0.16666667  0.          0.          0.
  0.          0.          0.85714286  0.          0.          0.51785714
  0.          0.          0.03571429  0.          0.82795699  0.
  0.31818182  0.          0.95        0.          0.          0.42857143
  0.          0.84459459  0.          0.          0.          0.          0.
  0.          0.          0.57251908  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.62569832  0.17647059  0.          0.          0.          0.          0.
  0.          0.04255319  0.          0.          0.2         0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.51724138  0.          0.1
  0.          0.          0.09090909  0.          0.          0.          0.
  0.2         0.          0.          0.71551724  0.05405405  0.69014085
  0.          0.07142857  0.        ], 
F1-measure: 
[ 0.87361111  0.          0.11111111  0.26315789  0.          0.          0.
  0.          0.          0.90566038  0.          0.          0.61052632
  0.          0.          0.06896552  0.          0.83923706  0.          0.4516129
  0.          0.9588785   0.          0.          0.57692308  0.
  0.90252708  0.          0.          0.          0.          0.          0.
  0.          0.68807339  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.68085106  0.29268293  0.          0.          0.          0.          0.
  0.          0.08        0.          0.          0.33333333  0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.66666667  0.
  0.18181818  0.          0.          0.16216216  0.          0.          0.
  0.          0.33333333  0.          0.          0.76146789  0.1025641
  0.77777778  0.          0.13333333  0.        ]




#########################################################################################
RCV-1 Set1


Num of train Docs:  23149
Num of test Docs:  199328
Load_Embedings :: GoogleVecs
Num of Docs :  222477
Num of words per Doc :  488
Number of unique Words :  4912
Words not found in embeddings :  217
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4912, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5],  							
Filter-Counts : [800, 800], 							
Learning Rate : 0.001, 								
Pool-Windows : [2, 2]
Nm of classes :  103
Input tensor shape:  (None, 488)
2018-05-08 00:18:28.192521: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-08 00:18:28.192568: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-08 00:18:28.192593: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 488, 300)
Convolution shape at loop  0  :  (None, 484, 800)
Max-Pool shape at loop  0  :  (None, 96, 800)
Convolution shape at loop  1  :  (None, 92, 800)
Max-Pool shape at loop  1  :  (None, 18, 800)
Train on 23149 samples, validate on 500 samples
Epoch 1/35
 - 682s - loss: 0.1200 - acc: 0.9663 - val_loss: 0.0798 - val_acc: 0.9776
Epoch 2/35
 - 702s - loss: 0.0971 - acc: 0.9714 - val_loss: 0.0725 - val_acc: 0.9786
Epoch 3/35
 - 737s - loss: 0.0935 - acc: 0.9718 - val_loss: 0.0707 - val_acc: 0.9789
Epoch 4/35
 - 732s - loss: 0.0949 - acc: 0.9721 - val_loss: 0.0659 - val_acc: 0.9808
Epoch 5/35
 - 708s - loss: 0.0845 - acc: 0.9744 - val_loss: 0.0592 - val_acc: 0.9820
Epoch 6/35
 - 705s - loss: 0.0809 - acc: 0.9757 - val_loss: 0.0571 - val_acc: 0.9826
Epoch 7/35
 - 705s - loss: 0.0728 - acc: 0.9775 - val_loss: 0.0553 - val_acc: 0.9833
Epoch 8/35
 - 704s - loss: 0.0686 - acc: 0.9785 - val_loss: 0.0547 - val_acc: 0.9833
Epoch 9/35
 - 704s - loss: 0.0653 - acc: 0.9793 - val_loss: 0.0565 - val_acc: 0.9833
Epoch 10/35
 - 705s - loss: 0.0620 - acc: 0.9800 - val_loss: 0.0531 - val_acc: 0.9842
Epoch 11/35
 - 702s - loss: 0.0593 - acc: 0.9808 - val_loss: 0.0561 - val_acc: 0.9837
Epoch 12/35
 - 707s - loss: 0.0570 - acc: 0.9814 - val_loss: 0.0564 - val_acc: 0.9840
Epoch 13/35
 - 705s - loss: 0.0549 - acc: 0.9820 - val_loss: 0.0552 - val_acc: 0.9844
Epoch 14/35
 - 703s - loss: 0.0528 - acc: 0.9827 - val_loss: 0.0573 - val_acc: 0.9842
Epoch 15/35
 - 707s - loss: 0.0506 - acc: 0.9832 - val_loss: 0.0554 - val_acc: 0.9842
Epoch 16/35
 - 706s - loss: 0.0488 - acc: 0.9837 - val_loss: 0.0577 - val_acc: 0.9847
Epoch 17/35
 - 706s - loss: 0.0469 - acc: 0.9843 - val_loss: 0.0598 - val_acc: 0.9847
Epoch 18/35
 - 709s - loss: 0.0456 - acc: 0.9847 - val_loss: 0.0589 - val_acc: 0.9848
Epoch 19/35
 - 709s - loss: 0.0442 - acc: 0.9851 - val_loss: 0.0596 - val_acc: 0.9844
Epoch 20/35
 - 705s - loss: 0.0430 - acc: 0.9856 - val_loss: 0.0621 - val_acc: 0.9850
Epoch 21/35
 - 709s - loss: 0.0418 - acc: 0.9859 - val_loss: 0.0610 - val_acc: 0.9849
Epoch 22/35
 - 708s - loss: 0.0408 - acc: 0.9862 - val_loss: 0.0644 - val_acc: 0.9850
Epoch 23/35
 - 709s - loss: 0.0399 - acc: 0.9865 - val_loss: 0.0637 - val_acc: 0.9848
Epoch 24/35
 - 713s - loss: 0.0386 - acc: 0.9869 - val_loss: 0.0665 - val_acc: 0.9846
Epoch 25/35
 - 708s - loss: 0.0382 - acc: 0.9870 - val_loss: 0.0657 - val_acc: 0.9848
Epoch 26/35
 - 708s - loss: 0.0372 - acc: 0.9873 - val_loss: 0.0681 - val_acc: 0.9853
Epoch 27/35
 - 710s - loss: 0.0367 - acc: 0.9875 - val_loss: 0.0700 - val_acc: 0.9850
Epoch 28/35
 - 709s - loss: 0.0355 - acc: 0.9879 - val_loss: 0.0714 - val_acc: 0.9847
Epoch 29/35
 - 711s - loss: 0.0350 - acc: 0.9880 - val_loss: 0.0724 - val_acc: 0.9848
Epoch 30/35
 - 709s - loss: 0.0344 - acc: 0.9882 - val_loss: 0.0722 - val_acc: 0.9851
Epoch 31/35
 - 707s - loss: 0.0337 - acc: 0.9885 - val_loss: 0.0724 - val_acc: 0.9852
Epoch 32/35
 - 710s - loss: 0.0334 - acc: 0.9885 - val_loss: 0.0714 - val_acc: 0.9852
Epoch 33/35
 - 709s - loss: 0.0327 - acc: 0.9888 - val_loss: 0.0724 - val_acc: 0.9851
Epoch 34/35
 - 714s - loss: 0.0324 - acc: 0.9888 - val_loss: 0.0748 - val_acc: 0.9850
Epoch 35/35
 - 710s - loss: 0.0317 - acc: 0.9891 - val_loss: 0.0767 - val_acc: 0.9850
Shape of 
1.test_labels :  (199328, 103) 
2.predictions :  (199328, 103)
Micro-average quality numbers
Precision: 0.8168, Recall: 0.4852, F1-measure: 0.6088
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
Macro-average quality numbers
Precision: 0.4514, Recall: 0.1521, F1-measure: 0.1966
All-Class quality numbers
Precision: 
[ 0.61585366  0.66666667  0.44827586  0.46153846  0.84147231  0.9311317
  0.22323462  0.67008535  0.          0.71399261  0.7702089   0.75        0.
  0.          0.70334198  0.69066228  0.          0.          0.60239163
  0.2         0.          0.64236453  0.64493997  0.          0.07692308
  0.          0.          0.58666667  0.          0.          0.82002384
  0.83088235  0.84279141  0.83419904  0.65        0.69672131  0.85714286
  0.64835165  0.71830986  0.          1.          0.          0.          0.
  0.74529823  0.73631124  0.86682108  0.          0.          0.          0.
  0.80366618  0.66666667  0.75186722  0.          0.80383481  0.          0.
  0.8938401   0.75848792  0.85        0.          0.          0.
  0.94594595  0.          0.          0.          0.          0.
  0.87039977  0.65180756  0.86567164  0.69136523  0.60052219  0.          0.2
  0.          0.27272727  0.80381584  0.          0.          0.
  0.64190894  0.39705882  0.          0.          0.94086022  0.
  0.65955077  0.73781903  0.85779817  1.          0.84214744  0.7663092
  0.79576378  0.77922742  0.85369422  0.87773676  0.83177794  0.87138264
  0.88033844  0.8410538 ], 
Recall: 
[  1.65140615e-02   6.33484163e-02   2.74117027e-03   9.24974306e-03
   6.73737964e-01   4.48409620e-01   4.86835569e-02   4.78833538e-01
   0.00000000e+00   1.51845711e-01   1.71417020e-01   3.19375444e-03
   0.00000000e+00   0.00000000e+00   3.55742513e-01   3.15245963e-01
   0.00000000e+00   0.00000000e+00   5.64346730e-02   5.80383053e-04
   0.00000000e+00   8.01278112e-02   3.57686454e-02   0.00000000e+00
   5.60224090e-04   0.00000000e+00   0.00000000e+00   1.14404576e-02
   0.00000000e+00   0.00000000e+00   2.53034204e-01   2.73387097e-01
   3.55203620e-01   8.42139090e-01   6.01573346e-03   3.88305162e-02
   1.06382979e-02   3.53505093e-02   3.57894737e-02   0.00000000e+00
   1.81159420e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00
   2.56685482e-01   1.39085465e-01   2.06596743e-01   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.90529770e-01
   3.92156863e-03   1.60098957e-01   0.00000000e+00   1.58016816e-01
   0.00000000e+00   0.00000000e+00   4.92418773e-01   4.28599684e-01
   8.97413338e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00
   1.56950673e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   8.04537810e-01   3.17028027e-01
   2.59623993e-02   2.37736985e-01   1.02267675e-01   0.00000000e+00
   3.37837838e-03   0.00000000e+00   1.91326531e-03   4.44087993e-01
   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.93476799e-01
   1.81573638e-02   0.00000000e+00   0.00000000e+00   7.42064454e-01
   0.00000000e+00   4.17361682e-01   2.29824139e-01   2.11060948e-01
   2.28310502e-03   5.80022075e-01   3.15663538e-01   4.69658084e-01
   4.73716952e-01   1.96729602e-01   7.02941027e-01   6.49598206e-01
   9.91584340e-02   4.18261868e-01   7.75316939e-01], 
F1-measure: 
[ 0.03216561  0.11570248  0.00544902  0.01813602  0.74832101  0.60531483
  0.07993475  0.55854132  0.          0.25043178  0.28042328  0.00636042
  0.          0.          0.47249987  0.43289932  0.          0.
  0.10320102  0.00115741  0.          0.14248252  0.06777828  0.
  0.00111235  0.          0.          0.02244325  0.          0.
  0.38673412  0.41140777  0.49977262  0.83815026  0.01192114  0.07356123
  0.02101576  0.06704545  0.06818182  0.          0.00361664  0.          0.
  0.          0.38185698  0.23397436  0.33366767  0.          0.          0.
  0.          0.52563496  0.00779727  0.26398601  0.          0.26411437
  0.          0.          0.63500931  0.54770631  0.01776075  0.          0.
  0.          0.03087781  0.          0.          0.          0.          0.
  0.83617388  0.42657654  0.05041286  0.35381055  0.17477204  0.
  0.00664452  0.          0.00379987  0.57210332  0.          0.          0.
  0.48788826  0.03472669  0.          0.          0.82972094  0.
  0.51122302  0.35047759  0.33876812  0.00455581  0.6869281   0.44713857
  0.59069138  0.58922526  0.31976983  0.7806742   0.72948583  0.17805519
  0.56709058  0.80684863]
Num of train docs per category:
 [  674   381   947   160  4179  2366   399  1930    49  1172   437   285
    76   246  1462  1205   142   202   793   190    62   922  1058   166
   196    41    43   443    31   120   312   286   343 10786   279   679
    94   187   167    17    65    12     8    34  1255   407   853    66
    43     0     3   449    40   641   102   400    54    15   166  3449
   363    49    35    59   138    38     2    45    52     2  6970  1133
   233  1004   293   106   172     6   197   471     0    13    90  1647
   166    92    37   913    23  1115   346   135    51  1294   732  1596
   943   699  2541  1508   311   606  5882]
Num of test docs per category:
 [ 6116  2652  9485  1946 39361 20624  2013 19512   438 11459  4947  2818
   623  1942 12956 10652  1131  1871  7141  1723   665  8137 10512  1085
  1785   238   464  3846   305  1178  2719  2480  3094 98023  2161  6567
   564  1669  1425   243   552    84    55   318 10807  3674  7246   591
   423    12    25  4266   510  5659   830  3449   491    98  1385 30336
  5683   958   594   625  2230   579    75   535  1140    19 60646  7564
  2234  9969  2249   987  1480    77  1568  4364     0   168   602 14870
  1487   721   542  8254   194  8513  4151   886   438 10872  5433 12079
  6430  5932 19619 10702  2733  5224 45829]
  



#########################################################################################
RCV-1 Set2


Num of train Docs:  23149
Num of test Docs:  199339
Load_Embedings :: GoogleVecs
Num of Docs :  222488
Num of words per Doc :  578
Number of unique Words :  4891
Words not found in embeddings :  205
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4891, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5],  							
Filter-Counts : [800, 800], 							
Learning Rate : 0.001, 								
Pool-Windows : [2, 2]
Nm of classes :  103
Input tensor shape:  (None, 578)
2018-05-08 12:36:29.398818: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-08 12:36:29.398876: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-08 12:36:29.398933: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 578, 300)
Convolution shape at loop  0  :  (None, 574, 800)
Max-Pool shape at loop  0  :  (None, 114, 800)
Convolution shape at loop  1  :  (None, 110, 800)
Max-Pool shape at loop  1  :  (None, 22, 800)
Train on 23149 samples, validate on 500 samples
Epoch 1/25
 - 618s - loss: 0.1178 - acc: 0.9664 - val_loss: 0.0836 - val_acc: 0.9762
Epoch 2/25
 - 621s - loss: 0.0966 - acc: 0.9715 - val_loss: 0.0809 - val_acc: 0.9768
Epoch 3/25
 - 623s - loss: 0.0921 - acc: 0.9721 - val_loss: 0.0750 - val_acc: 0.9775
Epoch 4/25
 - 622s - loss: 0.0840 - acc: 0.9744 - val_loss: 0.0689 - val_acc: 0.9804
Epoch 5/25
 - 621s - loss: 0.0780 - acc: 0.9762 - val_loss: 0.0649 - val_acc: 0.9809
Epoch 6/25
 - 622s - loss: 0.0733 - acc: 0.9774 - val_loss: 0.0605 - val_acc: 0.9829
Epoch 7/25
 - 622s - loss: 0.0687 - acc: 0.9783 - val_loss: 0.0598 - val_acc: 0.9834
Epoch 8/25
 - 621s - loss: 0.0658 - acc: 0.9790 - val_loss: 0.0603 - val_acc: 0.9836
Epoch 9/25
 - 622s - loss: 0.0640 - acc: 0.9797 - val_loss: 0.0611 - val_acc: 0.9834
Epoch 10/25
 - 621s - loss: 0.0601 - acc: 0.9805 - val_loss: 0.0627 - val_acc: 0.9831
Epoch 11/25
 - 621s - loss: 0.0578 - acc: 0.9811 - val_loss: 0.0638 - val_acc: 0.9839
Epoch 12/25
 - 622s - loss: 0.0557 - acc: 0.9816 - val_loss: 0.0639 - val_acc: 0.9835
Epoch 13/25
 - 622s - loss: 0.0538 - acc: 0.9822 - val_loss: 0.0643 - val_acc: 0.9839
Epoch 14/25
 - 626s - loss: 0.0518 - acc: 0.9828 - val_loss: 0.0652 - val_acc: 0.9839
Epoch 15/25
 - 627s - loss: 0.0502 - acc: 0.9833 - val_loss: 0.0652 - val_acc: 0.9841
Epoch 16/25
 - 629s - loss: 0.0489 - acc: 0.9837 - val_loss: 0.0621 - val_acc: 0.9847
Epoch 17/25
 - 629s - loss: 0.0473 - acc: 0.9841 - val_loss: 0.0671 - val_acc: 0.9840
Epoch 18/25
 - 629s - loss: 0.0459 - acc: 0.9845 - val_loss: 0.0707 - val_acc: 0.9839
Epoch 19/25
 - 630s - loss: 0.0449 - acc: 0.9850 - val_loss: 0.0698 - val_acc: 0.9838
Epoch 20/25
 - 629s - loss: 0.0434 - acc: 0.9853 - val_loss: 0.0685 - val_acc: 0.9845
Epoch 21/25
 - 630s - loss: 0.0425 - acc: 0.9857 - val_loss: 0.0689 - val_acc: 0.9846
Epoch 22/25
 - 630s - loss: 0.0419 - acc: 0.9858 - val_loss: 0.0710 - val_acc: 0.9843
Epoch 23/25
 - 622s - loss: 0.0407 - acc: 0.9862 - val_loss: 0.0729 - val_acc: 0.9842
Epoch 24/25
 - 627s - loss: 0.0395 - acc: 0.9866 - val_loss: 0.0729 - val_acc: 0.9845
Epoch 25/25
 - 631s - loss: 0.0389 - acc: 0.9867 - val_loss: 0.0714 - val_acc: 0.9841
Shape of 
1.test_labels :  (199339, 103) 
2.predictions :  (199339, 103)
Micro-average quality numbers
Precision: 0.8146, Recall: 0.4413, F1-measure: 0.5725
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
Macro-average quality numbers
Precision: 0.3830, Recall: 0.1270, F1-measure: 0.1669
All-Class quality numbers
Precision: 
[ 0.1         0.60169492  0.44642857  0.          0.84356766  0.89960151
  0.42105263  0.75414919  0.          0.6703668   0.72764228  0.          0.
  0.          0.69949223  0.69126254  0.          0.          0.37654321
  0.5         0.          0.68389423  0.54895105  0.          0.          0.
  0.          0.          0.          0.          0.85384615  0.84378844
  0.78708304  0.79704946  0.22222222  0.60164271  0.          0.96969697
  0.96875     0.          0.          0.          0.          0.
  0.69282946  0.73033708  0.7369186   0.          0.          0.          0.
  0.84590009  0.          0.68323442  0.          0.72612198  0.          0.
  0.83697479  0.67524255  0.77108434  0.          0.          0.
  0.81355932  0.          0.          0.          0.          0.
  0.89805705  0.72788412  0.25        0.71045392  0.58333333  0.          0.
  0.          0.5         0.83333333  0.          0.          0.          0.6359447
  0.53333333  0.          0.          0.92221804  0.          0.69646365
  0.47741935  0.88888889  0.          0.883834    0.82299887  0.76804767
  0.74904271  0.83641405  0.8786592   0.83235939  0.7         0.91816853
  0.87720652], 
Recall: 
[  1.53515505e-04   2.54662841e-02   2.58665287e-03   0.00000000e+00
   5.46010431e-01   4.77890156e-01   9.16380298e-04   2.43665292e-01
   0.00000000e+00   1.29065230e-01   1.20891490e-01   0.00000000e+00
   0.00000000e+00   0.00000000e+00   3.30401919e-01   2.90572006e-01
   0.00000000e+00   0.00000000e+00   1.90297925e-02   6.16903146e-04
   0.00000000e+00   6.83155241e-02   1.40580229e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   2.19947160e-01   2.47742867e-01
   3.25506311e-01   8.42683940e-01   8.71839582e-04   4.28237357e-02
   0.00000000e+00   1.83066362e-02   2.05298013e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   2.67414680e-01   1.32484076e-01   2.24833703e-01   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.74192203e-01
   0.00000000e+00   1.78522970e-01   0.00000000e+00   1.96573209e-01
   0.00000000e+00   0.00000000e+00   3.74717833e-01   5.03780779e-01
   1.20323369e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00
   2.34948605e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   7.61518274e-01   1.71334632e-01
   1.40646976e-03   1.11675676e-01   3.34750266e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   6.78886626e-04   4.13393964e-01
   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.70172004e-01
   4.41988950e-03   0.00000000e+00   0.00000000e+00   7.70383567e-01
   0.00000000e+00   3.34631268e-01   7.81002639e-02   9.86436498e-03
   0.00000000e+00   4.29644636e-01   1.23082111e-01   4.31966958e-01
   3.65636233e-01   1.46915584e-01   6.63347581e-01   6.01319261e-01
   4.95224620e-03   3.57820805e-01   7.19701767e-01], 
F1-measure: 
[  3.06560392e-04   4.88644184e-02   5.14350375e-03   0.00000000e+00
   6.62930344e-01   6.24193548e-01   1.82878043e-03   3.68324945e-01
   0.00000000e+00   2.16456288e-01   2.07335907e-01   0.00000000e+00
   0.00000000e+00   0.00000000e+00   4.48810347e-01   4.09155583e-01
   0.00000000e+00   0.00000000e+00   3.62286563e-02   1.23228589e-03
   0.00000000e+00   1.24222246e-01   2.74140038e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   3.49789916e-01   3.83026242e-01
   4.60548173e-01   8.19231685e-01   1.73686496e-03   7.99563378e-02
   0.00000000e+00   3.59348681e-02   4.02075227e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   3.85886798e-01   2.24282942e-01   3.44546381e-01   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.18861107e-01
   0.00000000e+00   2.83079760e-01   0.00000000e+00   3.09389556e-01
   0.00000000e+00   0.00000000e+00   5.17671518e-01   5.77044083e-01
   2.36949278e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00
   4.56707897e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   8.24170911e-01   2.77378019e-01
   2.79720280e-03   1.93011958e-01   6.33165829e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   1.35593220e-03   5.52638851e-01
   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.79232506e-01
   8.76712329e-03   0.00000000e+00   0.00000000e+00   8.39490664e-01
   0.00000000e+00   4.52060253e-01   1.34240363e-01   1.95121951e-02
   0.00000000e+00   5.78211973e-01   2.14139044e-01   5.52945284e-01
   4.91400966e-01   2.49930958e-01   7.55971327e-01   6.98223039e-01
   9.83491394e-03   5.14956973e-01   7.90686715e-01]
Num of train docs per category:
 [  674   381   947   160  4179  2366   399  1930    49  1172   437   285
    76   246  1462  1205   142   202   793   190    62   922  1058   166
   196    41    43   443    31   120   312   286   343 10786   279   679
    94   187   167    17    65    12     8    34  1255   407   853    66
    43     0     3   449    40   641   102   400    54    15   166  3449
   363    49    35    59   138    38     2    45    52     2  6970  1133
   233  1004   293   106   172     6   197   471     0    13    90  1647
   166    92    37   913    23  1115   346   135    51  1294   732  1596
   943   699  2541  1508   311   606  5882]
Num of test docs per category:
 [ 6514  2788  9665  1801 33362 17006  8730 16970   615 10762  4442  3045
   613  1734 13759 11381  1230  1880  6411  1621   673  8329 11168  1168
  1844   353   426  4003   337  1176  3028  2769  3407 93862  2294  6842
   581  1748  1510   226   557    92    46   312 10695  3925  6765   646
   441    19    32  4797   618  5159   718  3210   509   118  1329 30946
  5319   917   562   704  2043   602    88   470  1104    10 61728  8212
  2133  9250  1882   974  1479    46  1473  4838     0   292   809 14302
  1810   685   596  9542   193  8475  1895   811   513 11847  5931 12832
  6955  6160 20588 11370  2827  5268 48955]




#########################################################################################
RCV-1 Set3






#########################################################################################
RCV-1 Set4


