################################################################################
20 NewsGroups:

KerasBlog_CNN_Classifier
filter_sizes=[5,5,5]
filter_counts=[128,128,128]
learning_rate=0.001
batch_size=128
num_epochs=40
Words=5000

RESULTS:
Total Doc Count :  18846
Label dimention :  (11314, 20)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  18846
Num of words per Doc :  12013
Number of unique Words :  4998
Words not found in embeddings :  2109
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4998, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5, 5],  							
Filter-Counts : [128, 128, 128], 						
Pool-Windows : [5, 5, 5]
Nm of classes :  20
Input tensor shape:  (None, 1000)
2018-03-28 21:12:06.495334: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-28 21:12:06.495404: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-28 21:12:06.495452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 1000, 300)
Convolution shape at loop  0  :  (None, 996, 128)
Max-Pool shape at loop  0  :  (None, 199, 128)
Convolution shape at loop  1  :  (None, 195, 128)
Max-Pool shape at loop  1  :  (None, 39, 128)
Convolution shape at loop  2  :  (None, 35, 128)
Max-Pool shape at loop  2  :  (None, 7, 128)
Train on 9051 samples, validate on 2263 samples
Epoch 1/40
 - 67s - loss: 2.9728 - acc: 0.0679 - val_loss: 2.8762 - val_acc: 0.0884
Epoch 2/40
 - 67s - loss: 2.6484 - acc: 0.1248 - val_loss: 2.2032 - val_acc: 0.2762
Epoch 3/40
 - 67s - loss: 1.9970 - acc: 0.2845 - val_loss: 1.5584 - val_acc: 0.4264
Epoch 4/40
 - 67s - loss: 1.4886 - acc: 0.4320 - val_loss: 1.3005 - val_acc: 0.5320
Epoch 5/40
 - 67s - loss: 1.1982 - acc: 0.5555 - val_loss: 1.1320 - val_acc: 0.5868
Epoch 6/40
 - 67s - loss: 0.9782 - acc: 0.6256 - val_loss: 1.0264 - val_acc: 0.6381
Epoch 7/40
 - 67s - loss: 0.8537 - acc: 0.6806 - val_loss: 0.9789 - val_acc: 0.6920
Epoch 8/40
 - 67s - loss: 0.7130 - acc: 0.7390 - val_loss: 1.0219 - val_acc: 0.6982
Epoch 9/40
 - 67s - loss: 0.6073 - acc: 0.7871 - val_loss: 1.0187 - val_acc: 0.7212
Epoch 10/40
 - 67s - loss: 0.4836 - acc: 0.8373 - val_loss: 0.8684 - val_acc: 0.7627
Epoch 11/40
 - 66s - loss: 0.4065 - acc: 0.8673 - val_loss: 0.8147 - val_acc: 0.7941
Epoch 12/40
 - 67s - loss: 0.3469 - acc: 0.8895 - val_loss: 0.8308 - val_acc: 0.7817
Epoch 13/40
 - 66s - loss: 0.2907 - acc: 0.9047 - val_loss: 0.8435 - val_acc: 0.7998
Epoch 14/40
 - 67s - loss: 0.2413 - acc: 0.9241 - val_loss: 0.8425 - val_acc: 0.8118
Epoch 15/40
 - 67s - loss: 0.2097 - acc: 0.9360 - val_loss: 0.8859 - val_acc: 0.8047
Epoch 16/40
 - 67s - loss: 0.1864 - acc: 0.9453 - val_loss: 0.8780 - val_acc: 0.8060
Epoch 17/40
 - 67s - loss: 0.1535 - acc: 0.9514 - val_loss: 1.0010 - val_acc: 0.7976
Epoch 18/40
 - 67s - loss: 0.1496 - acc: 0.9576 - val_loss: 0.9715 - val_acc: 0.8016
Epoch 19/40
 - 67s - loss: 0.1413 - acc: 0.9582 - val_loss: 1.0071 - val_acc: 0.8175
Epoch 20/40
 - 67s - loss: 0.1384 - acc: 0.9580 - val_loss: 0.9922 - val_acc: 0.8051
Epoch 21/40
 - 67s - loss: 0.1483 - acc: 0.9540 - val_loss: 0.9122 - val_acc: 0.8237
Epoch 22/40
 - 67s - loss: 0.0968 - acc: 0.9737 - val_loss: 1.0006 - val_acc: 0.8171
Epoch 23/40
 - 67s - loss: 0.0995 - acc: 0.9753 - val_loss: 1.0042 - val_acc: 0.8277
Epoch 24/40
 - 67s - loss: 0.0843 - acc: 0.9772 - val_loss: 1.0654 - val_acc: 0.8144
Epoch 25/40
 - 67s - loss: 0.0881 - acc: 0.9745 - val_loss: 1.1007 - val_acc: 0.8188
Epoch 26/40
 - 67s - loss: 0.0759 - acc: 0.9790 - val_loss: 1.1218 - val_acc: 0.8162
Epoch 27/40
 - 67s - loss: 0.0696 - acc: 0.9793 - val_loss: 1.2956 - val_acc: 0.8003
Epoch 28/40
 - 68s - loss: 0.0724 - acc: 0.9779 - val_loss: 1.2382 - val_acc: 0.8091
Epoch 29/40
 - 67s - loss: 0.0641 - acc: 0.9821 - val_loss: 1.1315 - val_acc: 0.8312
Epoch 30/40
 - 67s - loss: 0.0790 - acc: 0.9774 - val_loss: 1.0880 - val_acc: 0.8263
Epoch 31/40
 - 67s - loss: 0.0517 - acc: 0.9854 - val_loss: 1.1405 - val_acc: 0.8281
Epoch 32/40
 - 67s - loss: 0.0645 - acc: 0.9815 - val_loss: 1.2242 - val_acc: 0.8219
Epoch 33/40
 - 67s - loss: 0.0539 - acc: 0.9849 - val_loss: 1.3363 - val_acc: 0.8025
Epoch 34/40
 - 67s - loss: 0.0483 - acc: 0.9873 - val_loss: 1.2468 - val_acc: 0.8197
Epoch 35/40
 - 67s - loss: 0.0577 - acc: 0.9833 - val_loss: 1.1921 - val_acc: 0.8215
Epoch 36/40
 - 67s - loss: 0.0438 - acc: 0.9874 - val_loss: 1.2003 - val_acc: 0.8281
Epoch 37/40
 - 67s - loss: 0.0433 - acc: 0.9876 - val_loss: 1.2197 - val_acc: 0.8224
Epoch 38/40
 - 67s - loss: 0.0464 - acc: 0.9876 - val_loss: 1.1680 - val_acc: 0.8255
Epoch 39/40
 - 67s - loss: 0.0449 - acc: 0.9876 - val_loss: 1.1578 - val_acc: 0.8343
Epoch 40/40
 - 67s - loss: 0.0429 - acc: 0.9884 - val_loss: 1.2542 - val_acc: 0.8263
Micro-average quality numbers
Precision: 0.7272, Recall: 0.7272, F1-measure: 0.7272
Macro-average quality numbers
Precision: 0.7255, Recall: 0.7205, F1-measure: 0.7193
All-Class quality numbers
Precision: 
[ 0.58536585  0.63209877  0.73898305  0.61696658  0.66815145  0.70646766
  0.61333333  0.77624309  0.81333333  0.84473684  0.90096618  0.9093199
  0.59411765  0.81707317  0.80541872  0.79746835  0.63656388  0.95517241
  0.54754098  0.54976303], 
Recall: 
[ 0.7523511   0.65809769  0.55329949  0.6122449   0.77922078  0.71898734
  0.70769231  0.70959596  0.91959799  0.80856423  0.93483709  0.91161616
  0.51399491  0.67676768  0.82994924  0.79145729  0.79395604  0.73670213
  0.53870968  0.46215139], 
F1-measure: 
[ 0.65843621  0.64483627  0.63280116  0.61459667  0.71942446  0.71267252
  0.65714286  0.7414248   0.86320755  0.82625483  0.91758918  0.91046658
  0.55115962  0.74033149  0.8175      0.79445145  0.70660147  0.83183183
  0.54308943  0.5021645 ]
  
  
  
  
  
  
  
KerasBlog_CNN_Classifier
filter_sizes=[5,5,5]
filter_counts=[200,200,200]
learning_rate=0.001
batch_size=128
num_epochs=50
Words=5000

RESULTS:
Total Doc Count :  18846
Label dimention :  (11314, 20)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  18846
Num of words per Doc :  12013
Number of unique Words :  4998
Words not found in embeddings :  2109
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4998, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5, 5],  							
Filter-Counts : [200, 200, 200], 						
Pool-Windows : [5, 5, 5]
Nm of classes :  20
Input tensor shape:  (None, 1000)
2018-03-29 10:32:41.297193: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-29 10:32:41.297241: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-29 10:32:41.297267: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 1000, 300)
Convolution shape at loop  0  :  (None, 996, 200)
Max-Pool shape at loop  0  :  (None, 199, 200)
Convolution shape at loop  1  :  (None, 195, 200)
Max-Pool shape at loop  1  :  (None, 39, 200)
Convolution shape at loop  2  :  (None, 35, 200)
Max-Pool shape at loop  2  :  (None, 7, 200)
Train on 10182 samples, validate on 1132 samples
Epoch 1/50
 - 145s - loss: 2.9402 - acc: 0.0767 - val_loss: 2.6927 - val_acc: 0.1078
Epoch 2/50
 - 169s - loss: 2.4123 - acc: 0.1768 - val_loss: 1.8301 - val_acc: 0.3675
Epoch 3/50
 - 168s - loss: 1.6351 - acc: 0.4091 - val_loss: 1.3621 - val_acc: 0.5124
Epoch 4/50
 - 169s - loss: 1.1473 - acc: 0.5709 - val_loss: 1.0505 - val_acc: 0.6519
Epoch 5/50
 - 168s - loss: 0.8336 - acc: 0.6959 - val_loss: 0.8952 - val_acc: 0.7120
Epoch 6/50
 - 169s - loss: 0.6367 - acc: 0.7812 - val_loss: 0.8516 - val_acc: 0.7438
Epoch 7/50
 - 168s - loss: 0.4830 - acc: 0.8422 - val_loss: 0.7320 - val_acc: 0.7906
Epoch 8/50
 - 168s - loss: 0.3681 - acc: 0.8838 - val_loss: 0.7047 - val_acc: 0.8004
Epoch 9/50
 - 169s - loss: 0.2949 - acc: 0.9068 - val_loss: 0.8207 - val_acc: 0.7906
Epoch 10/50
 - 126s - loss: 0.2140 - acc: 0.9340 - val_loss: 0.7574 - val_acc: 0.8136
Epoch 11/50
 - 116s - loss: 0.1930 - acc: 0.9446 - val_loss: 0.7490 - val_acc: 0.8136
Epoch 12/50
 - 116s - loss: 0.1642 - acc: 0.9519 - val_loss: 0.8089 - val_acc: 0.8189
Epoch 13/50
 - 116s - loss: 0.1275 - acc: 0.9624 - val_loss: 0.8376 - val_acc: 0.8189
Epoch 14/50
 - 116s - loss: 0.1115 - acc: 0.9663 - val_loss: 0.8615 - val_acc: 0.8198
Epoch 15/50
 - 116s - loss: 0.1063 - acc: 0.9707 - val_loss: 0.9132 - val_acc: 0.8180
Epoch 16/50
 - 116s - loss: 0.1035 - acc: 0.9718 - val_loss: 0.9968 - val_acc: 0.8163
Epoch 17/50
 - 116s - loss: 0.0896 - acc: 0.9758 - val_loss: 0.9552 - val_acc: 0.8286
Epoch 18/50
 - 116s - loss: 0.0659 - acc: 0.9804 - val_loss: 0.9099 - val_acc: 0.8366
Epoch 19/50
 - 116s - loss: 0.0641 - acc: 0.9830 - val_loss: 0.9170 - val_acc: 0.8428
Epoch 20/50
 - 116s - loss: 0.0601 - acc: 0.9843 - val_loss: 0.9291 - val_acc: 0.8339
Epoch 21/50
 - 116s - loss: 0.0686 - acc: 0.9809 - val_loss: 1.0119 - val_acc: 0.8313
Epoch 22/50
 - 116s - loss: 0.0674 - acc: 0.9815 - val_loss: 0.8976 - val_acc: 0.8481
Epoch 23/50
 - 116s - loss: 0.0427 - acc: 0.9891 - val_loss: 0.9324 - val_acc: 0.8534
Epoch 24/50
 - 116s - loss: 0.0506 - acc: 0.9865 - val_loss: 1.0090 - val_acc: 0.8419
Epoch 25/50
 - 116s - loss: 0.0511 - acc: 0.9848 - val_loss: 0.9549 - val_acc: 0.8419
Epoch 26/50
 - 116s - loss: 0.0406 - acc: 0.9896 - val_loss: 1.0545 - val_acc: 0.8436
Epoch 27/50
 - 116s - loss: 0.0397 - acc: 0.9897 - val_loss: 1.0217 - val_acc: 0.8481
Epoch 28/50
 - 116s - loss: 0.0491 - acc: 0.9863 - val_loss: 0.9527 - val_acc: 0.8419
Epoch 29/50
 - 116s - loss: 0.0383 - acc: 0.9904 - val_loss: 0.9857 - val_acc: 0.8489
Epoch 30/50
 - 116s - loss: 0.0541 - acc: 0.9850 - val_loss: 1.2442 - val_acc: 0.8101
Epoch 31/50
 - 116s - loss: 0.0520 - acc: 0.9855 - val_loss: 1.0740 - val_acc: 0.8357
Epoch 32/50
 - 116s - loss: 0.0496 - acc: 0.9870 - val_loss: 1.0679 - val_acc: 0.8366
Epoch 33/50
 - 116s - loss: 0.0420 - acc: 0.9891 - val_loss: 1.1593 - val_acc: 0.8154
Epoch 34/50
 - 116s - loss: 0.0499 - acc: 0.9855 - val_loss: 1.0361 - val_acc: 0.8366
Epoch 35/50
 - 116s - loss: 0.0413 - acc: 0.9896 - val_loss: 0.9961 - val_acc: 0.8489
Epoch 36/50
 - 116s - loss: 0.0287 - acc: 0.9930 - val_loss: 1.0492 - val_acc: 0.8410
Epoch 37/50
 - 116s - loss: 0.0318 - acc: 0.9914 - val_loss: 1.0191 - val_acc: 0.8516
Epoch 38/50
 - 116s - loss: 0.0287 - acc: 0.9929 - val_loss: 0.9769 - val_acc: 0.8542
Epoch 39/50
 - 116s - loss: 0.0256 - acc: 0.9927 - val_loss: 1.0547 - val_acc: 0.8410
Epoch 40/50
 - 116s - loss: 0.0245 - acc: 0.9939 - val_loss: 1.2507 - val_acc: 0.8260
Epoch 41/50
 - 116s - loss: 0.0460 - acc: 0.9888 - val_loss: 1.0886 - val_acc: 0.8392
Epoch 42/50
 - 117s - loss: 0.0461 - acc: 0.9881 - val_loss: 0.9850 - val_acc: 0.8595
Epoch 43/50
 - 116s - loss: 0.0360 - acc: 0.9906 - val_loss: 0.9863 - val_acc: 0.8436
Epoch 44/50
 - 116s - loss: 0.0345 - acc: 0.9919 - val_loss: 1.1170 - val_acc: 0.8489
Epoch 45/50
 - 116s - loss: 0.0366 - acc: 0.9910 - val_loss: 0.9899 - val_acc: 0.8534
Epoch 46/50
 - 116s - loss: 0.0255 - acc: 0.9933 - val_loss: 1.1046 - val_acc: 0.8428
Epoch 47/50
 - 116s - loss: 0.0286 - acc: 0.9916 - val_loss: 1.0595 - val_acc: 0.8498
Epoch 48/50
 - 116s - loss: 0.0322 - acc: 0.9919 - val_loss: 1.2487 - val_acc: 0.8330
Epoch 49/50
 - 115s - loss: 0.0349 - acc: 0.9906 - val_loss: 1.1454 - val_acc: 0.8481
Epoch 50/50
 - 117s - loss: 0.0166 - acc: 0.9956 - val_loss: 1.0347 - val_acc: 0.8551
Micro-average quality numbers
Precision: 0.7492, Recall: 0.7492, F1-measure: 0.7492
Macro-average quality numbers
Precision: 0.7477, Recall: 0.7429, F1-measure: 0.7434
All-Class quality numbers
Precision: 
[ 0.71692308  0.71348315  0.6503856   0.57238307  0.65426696  0.79705882
  0.61894737  0.75682382  0.90591398  0.95428571  0.94162437  0.88366337
  0.63663664  0.82        0.84102564  0.79086538  0.69230769  0.8852459
  0.59859155  0.524     ], 
Recall: 
[ 0.73040752  0.6529563   0.64213198  0.65561224  0.77662338  0.68607595
  0.75384615  0.77020202  0.84673367  0.84130982  0.92982456  0.90151515
  0.5394402   0.72474747  0.83248731  0.82663317  0.81593407  0.86170213
  0.5483871   0.52191235], 
F1-measure: 
[ 0.72360248  0.68187919  0.64623244  0.61117717  0.71021378  0.73741497
  0.67976879  0.76345432  0.87532468  0.89424364  0.93568726  0.8925
  0.58402204  0.769437    0.83673469  0.80835381  0.74905422  0.87331536
  0.57239057  0.52295409]




#########################################################################################
Reuters 21578

## Multiclass ##
KerasBlog_CNN_Classifier
filter_sizes=[5,5]
filter_counts=[200,200]
learning_rate=0.001
batch_size=64
num_epochs=10
Dropout=0.8
l1(0.01) & l2(0.01) regularization

RESULTS:
Label dimention :  (5735, 10)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  7980
Num of words per Doc :  262
Number of unique Words :  501
Words not found in embeddings :  69
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (501, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5],  							
Filter-Counts : [200, 200], 							
Learning Rate : 0.001, 								
Pool-Windows : [2, 2]
Nm of classes :  10
Input tensor shape:  (None, 262)
2018-04-04 12:26:16.122653: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-04 12:26:16.122738: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-04 12:26:16.122766: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 262, 300)
Convolution shape at loop  0  :  (None, 258, 200)
Max-Pool shape at loop  0  :  (None, 51, 200)
Convolution shape at loop  1  :  (None, 47, 200)
Max-Pool shape at loop  1  :  (None, 9, 200)
Train on 5735 samples, validate on 2245 samples
Epoch 1/15
 - 17s - loss: 2.8819 - acc: 0.5226 - val_loss: 2.2022 - val_acc: 0.7751
Epoch 2/15
 - 16s - loss: 2.1802 - acc: 0.7519 - val_loss: 1.9009 - val_acc: 0.8347
Epoch 3/15
 - 17s - loss: 1.9580 - acc: 0.8099 - val_loss: 1.7355 - val_acc: 0.8820
Epoch 4/15
 - 17s - loss: 1.8130 - acc: 0.8521 - val_loss: 1.6366 - val_acc: 0.9105
Epoch 5/15
 - 17s - loss: 1.6907 - acc: 0.8884 - val_loss: 1.5582 - val_acc: 0.9269
Epoch 6/15
 - 16s - loss: 1.6148 - acc: 0.9137 - val_loss: 1.5268 - val_acc: 0.9394
Epoch 7/15
 - 17s - loss: 1.5551 - acc: 0.9289 - val_loss: 1.5162 - val_acc: 0.9448
Epoch 8/15
 - 16s - loss: 1.5198 - acc: 0.9458 - val_loss: 1.4793 - val_acc: 0.9510
Epoch 9/15
 - 17s - loss: 1.4875 - acc: 0.9522 - val_loss: 1.4701 - val_acc: 0.9590
Epoch 10/15
 - 17s - loss: 1.4607 - acc: 0.9594 - val_loss: 1.4587 - val_acc: 0.9612
Epoch 11/15
 - 19s - loss: 1.4441 - acc: 0.9622 - val_loss: 1.4722 - val_acc: 0.9563
Epoch 12/15
 - 20s - loss: 1.4221 - acc: 0.9705 - val_loss: 1.4498 - val_acc: 0.9653
Epoch 13/15
 - 17s - loss: 1.4170 - acc: 0.9700 - val_loss: 1.4711 - val_acc: 0.9604
Epoch 14/15
 - 16s - loss: 1.4052 - acc: 0.9726 - val_loss: 1.4541 - val_acc: 0.9635
Epoch 15/15
 - 16s - loss: 1.3872 - acc: 0.9801 - val_loss: 1.4493 - val_acc: 0.9657
Shape of 
1.test_labels :  (2245, 10) 
2.predictions :  (2245, 10)
Micro-average quality numbers
Precision: 0.9657, Recall: 0.9657, F1-measure: 0.9657
Macro-average quality numbers
Precision: 0.9204, Recall: 0.9265, F1-measure: 0.9232
All-Class quality numbers
Precision: 
[ 0.98115942  0.95454545  0.95689655  0.98795181  0.83544304  0.81395349
  0.92857143  0.89473684  0.96        0.8902439 ], 
Recall: 
[ 0.97410072  0.95454545  0.93277311  0.99070632  0.81481481  0.8045977
  0.92857143  0.94444444  0.96        0.96052632], 
F1-measure: 
[ 0.97761733  0.95454545  0.94468085  0.98932715  0.825       0.80924855
  0.92857143  0.91891892  0.96        0.92405063]
  
  
  
  
  
  
  
  
## Multi-label ##
Label dimention :  (7722, 90)
Starting data Preprocessing
Completed data Preprocessing
Load_Embedings :: GoogleVecs
Num of Docs :  10727
Num of words per Doc :  312
Number of unique Words :  501
Words not found in embeddings :  82
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (501, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5],  							
Filter-Counts : [300, 300], 							
Learning Rate : 0.001, 								
Pool-Windows : [2, 2]
Nm of classes :  90
Input tensor shape:  (None, 312)
2018-04-05 15:56:42.658726: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-05 15:56:42.658801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-05 15:56:42.658818: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 312, 300)
Convolution shape at loop  0  :  (None, 308, 300)
Max-Pool shape at loop  0  :  (None, 61, 300)
Convolution shape at loop  1  :  (None, 57, 300)
Max-Pool shape at loop  1  :  (None, 11, 300)
Train on 7722 samples, validate on 3005 samples
Epoch 1/17
 - 46s - loss: 0.0866 - acc: 0.9721 - val_loss: 0.0410 - val_acc: 0.9896
Epoch 2/17
 - 45s - loss: 0.0440 - acc: 0.9893 - val_loss: 0.0380 - val_acc: 0.9913
Epoch 3/17
 - 51s - loss: 0.0399 - acc: 0.9902 - val_loss: 0.0360 - val_acc: 0.9916
Epoch 4/17
 - 45s - loss: 0.0363 - acc: 0.9909 - val_loss: 0.0315 - val_acc: 0.9923
Epoch 5/17
 - 45s - loss: 0.0328 - acc: 0.9916 - val_loss: 0.0289 - val_acc: 0.9929
Epoch 6/17
 - 45s - loss: 0.0314 - acc: 0.9919 - val_loss: 0.0278 - val_acc: 0.9932
Epoch 7/17
 - 45s - loss: 0.0293 - acc: 0.9923 - val_loss: 0.0280 - val_acc: 0.9931
Epoch 8/17
 - 51s - loss: 0.0281 - acc: 0.9925 - val_loss: 0.0273 - val_acc: 0.9933
Epoch 9/17
 - 44s - loss: 0.0272 - acc: 0.9928 - val_loss: 0.0254 - val_acc: 0.9936
Epoch 10/17
 - 45s - loss: 0.0262 - acc: 0.9930 - val_loss: 0.0251 - val_acc: 0.9936
Epoch 11/17
 - 45s - loss: 0.0255 - acc: 0.9931 - val_loss: 0.0247 - val_acc: 0.9938
Epoch 12/17
 - 46s - loss: 0.0248 - acc: 0.9931 - val_loss: 0.0248 - val_acc: 0.9936
Epoch 13/17
 - 51s - loss: 0.0239 - acc: 0.9934 - val_loss: 0.0245 - val_acc: 0.9937
Epoch 14/17
 - 46s - loss: 0.0234 - acc: 0.9935 - val_loss: 0.0234 - val_acc: 0.9941
Epoch 15/17
 - 46s - loss: 0.0227 - acc: 0.9936 - val_loss: 0.0238 - val_acc: 0.9939
Epoch 16/17
 - 46s - loss: 0.0221 - acc: 0.9938 - val_loss: 0.0233 - val_acc: 0.9941
Epoch 17/17
 - 46s - loss: 0.0218 - acc: 0.9938 - val_loss: 0.0229 - val_acc: 0.9941
Type of predictions <class 'numpy.ndarray'>
Shape of 
1.test_labels :  (3005, 90) 
2.predictions :  (3005, 90)
Micro-average quality numbers
Precision: 0.9018, Recall: 0.6453, F1-measure: 0.7523
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
Macro-average quality numbers
Precision: 0.2269, Recall: 0.1106, F1-measure: 0.1306
All-Class quality numbers
Precision: 
[ 0.87119114  0.          0.25        0.625       0.          0.          0.
  0.          0.          0.96        0.          0.          0.74358974
  0.          0.          1.          0.          0.85082873  0.
  0.77777778  0.          0.96792453  0.          0.          0.88235294
  0.          0.96899225  0.          0.          0.          0.          0.
  0.          0.          0.86206897  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.74666667  0.85714286  0.          0.          0.          0.          0.
  0.          0.66666667  0.          0.          1.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.9375      0.          1.
  0.          0.          0.75        0.          0.          0.          0.
  1.          0.          0.          0.81372549  1.          0.89090909
  0.          1.          0.        ], 
Recall: 
[ 0.87604457  0.          0.07142857  0.16666667  0.          0.          0.
  0.          0.          0.85714286  0.          0.          0.51785714
  0.          0.          0.03571429  0.          0.82795699  0.
  0.31818182  0.          0.95        0.          0.          0.42857143
  0.          0.84459459  0.          0.          0.          0.          0.
  0.          0.          0.57251908  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.62569832  0.17647059  0.          0.          0.          0.          0.
  0.          0.04255319  0.          0.          0.2         0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.51724138  0.          0.1
  0.          0.          0.09090909  0.          0.          0.          0.
  0.2         0.          0.          0.71551724  0.05405405  0.69014085
  0.          0.07142857  0.        ], 
F1-measure: 
[ 0.87361111  0.          0.11111111  0.26315789  0.          0.          0.
  0.          0.          0.90566038  0.          0.          0.61052632
  0.          0.          0.06896552  0.          0.83923706  0.          0.4516129
  0.          0.9588785   0.          0.          0.57692308  0.
  0.90252708  0.          0.          0.          0.          0.          0.
  0.          0.68807339  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.68085106  0.29268293  0.          0.          0.          0.          0.
  0.          0.08        0.          0.          0.33333333  0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.66666667  0.
  0.18181818  0.          0.          0.16216216  0.          0.          0.
  0.          0.33333333  0.          0.          0.76146789  0.1025641
  0.77777778  0.          0.13333333  0.        ]




#########################################################################################
RCV-1
  
  
  Num of train Docs:  23149
Num of test Docs:  199328
Load_Embedings :: GoogleVecs
Num of Docs :  222477
Num of words per Doc :  488
Number of unique Words :  4912
Words not found in embeddings :  217
Load_Embedings :: GoogleVecs
Using TensorFlow backend.
Embeddings Shape :  (4912, 300)
Using Nested CNN with parameters : 
Batch-size : 128,  								
Filter-Sizes : [5, 5],  							
Filter-Counts : [400, 400], 							
Learning Rate : 0.001, 								
Pool-Windows : [2, 2]
50 epochs with random initialization of embeddings.
Nm of classes :  103
Input tensor shape:  (None, 488)
2018-05-05 00:12:41.815385: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-05 00:12:41.815439: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-05 00:12:41.815455: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Embeddings tensor shape:  (None, 488, 300)
Convolution shape at loop  0  :  (None, 484, 400)
Max-Pool shape at loop  0  :  (None, 96, 400)
Convolution shape at loop  1  :  (None, 92, 400)
Max-Pool shape at loop  1  :  (None, 18, 400)
Train on 23149 samples, validate on 199328 samples
Epoch 1/15
 - 571s - loss: 0.1140 - acc: 0.9665 - val_loss: 0.0969 - val_acc: 0.9715
Epoch 2/15
 - 571s - loss: 0.0951 - acc: 0.9720 - val_loss: 0.1016 - val_acc: 0.9731
Epoch 3/15
 - 571s - loss: 0.0862 - acc: 0.9745 - val_loss: 0.0870 - val_acc: 0.9746
Epoch 4/15
 - 570s - loss: 0.0812 - acc: 0.9757 - val_loss: 0.0777 - val_acc: 0.9764
Epoch 5/15
 - 571s - loss: 0.0765 - acc: 0.9767 - val_loss: 0.0753 - val_acc: 0.9769
Epoch 6/15
 - 570s - loss: 0.0716 - acc: 0.9777 - val_loss: 0.0721 - val_acc: 0.9778
Epoch 7/15
 - 572s - loss: 0.0679 - acc: 0.9786 - val_loss: 0.0720 - val_acc: 0.9779
Epoch 8/15
 - 571s - loss: 0.0650 - acc: 0.9792 - val_loss: 0.0711 - val_acc: 0.9784
Epoch 9/15
 - 571s - loss: 0.0624 - acc: 0.9798 - val_loss: 0.0698 - val_acc: 0.9787
Epoch 10/15
 - 571s - loss: 0.0605 - acc: 0.9804 - val_loss: 0.0720 - val_acc: 0.9786
Epoch 11/15
 - 572s - loss: 0.0583 - acc: 0.9809 - val_loss: 0.0729 - val_acc: 0.9790
Epoch 12/15
 - 571s - loss: 0.0567 - acc: 0.9813 - val_loss: 0.0714 - val_acc: 0.9791
Epoch 13/15
 - 572s - loss: 0.0550 - acc: 0.9819 - val_loss: 0.0730 - val_acc: 0.9791
Epoch 14/15
 - 572s - loss: 0.0536 - acc: 0.9821 - val_loss: 0.0731 - val_acc: 0.9793
Epoch 15/15
 - 571s - loss: 0.0521 - acc: 0.9827 - val_loss: 0.0726 - val_acc: 0.9795
Shape of 
1.test_labels :  (199328, 103) 
2.predictions :  (199328, 103)
Micro-average quality numbers
Precision: 0.8264, Recall: 0.4402, F1-measure: 0.5744
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/sounak/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
Macro-average quality numbers
Precision: 0.3416, Recall: 0.1111, F1-measure: 0.1435
All-Class quality numbers
Precision: 
[ 0.32258065  0.61538462  0.40909091  0.          0.90027149  0.80717185
  0.          0.75559176  0.          0.75        0.72727273  0.          0.
  0.          0.69693593  0.68862571  0.          0.          0.5786802   0.
  0.          0.55913978  0.33766234  0.          0.5         0.          0.
  0.          0.          0.          0.          0.          0.84300899
  0.83730461  1.          0.45052632  0.          0.          0.          0.
  0.          0.          0.          0.          0.87526728  0.75659229
  0.97590361  0.          0.          0.          0.          0.81138211
  0.          0.79355783  0.          0.79074447  0.          0.
  0.75366569  0.68424852  0.45454545  0.          0.          0.          0.4084507
  0.          0.          0.          0.          0.          0.88886577
  0.66375698  0.64        0.68551069  0.77906977  0.          0.          0.
  0.25        0.84354092  0.          0.          0.          0.65745779
  0.          0.          0.          0.94368463  0.          0.7019475
  0.76356589  0.87058824  0.          0.85624608  0.64233577  0.73369257
  0.68747783  0.66209262  0.88611168  0.83018598  0.38515901  0.8804458
  0.85374924], 
Recall: 
[  1.63505559e-03   3.92156863e-02   9.48866632e-04   0.00000000e+00
   6.31843703e-01   5.20607060e-01   0.00000000e+00   2.61428864e-01
   0.00000000e+00   2.61802950e-04   1.61714170e-03   0.00000000e+00
   0.00000000e+00   0.00000000e+00   2.89672738e-01   2.62016523e-01
   0.00000000e+00   0.00000000e+00   1.59641507e-02   0.00000000e+00
   0.00000000e+00   1.27811233e-02   2.47336377e-03   0.00000000e+00
   5.60224090e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   3.33225598e-01   8.39935525e-01   2.77649236e-03   3.25871783e-02
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   1.13630055e-01   1.01524224e-01   1.11785813e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.50914205e-01
   0.00000000e+00   9.57766390e-02   0.00000000e+00   1.13946071e-01
   0.00000000e+00   0.00000000e+00   1.85559567e-01   4.99901108e-01
   3.51926799e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00
   1.30044843e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   7.74939815e-01   2.51322052e-01
   7.16204118e-03   1.44748721e-01   2.97910182e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   6.37755102e-04   3.75572869e-01
   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.16879623e-01
   0.00000000e+00   0.00000000e+00   0.00000000e+00   7.10564575e-01
   0.00000000e+00   2.92141431e-01   1.89833775e-01   8.35214447e-02
   0.00000000e+00   5.01839588e-01   6.47892509e-02   4.28346717e-01
   3.01399689e-01   6.50708024e-02   6.55945767e-01   5.42235096e-01
   3.98829126e-02   1.66347626e-01   7.61461084e-01], 
F1-measure: 
[  3.25361965e-03   7.37327189e-02   1.89334175e-03   0.00000000e+00
   7.42543218e-01   6.32965867e-01   0.00000000e+00   3.88455241e-01
   0.00000000e+00   5.23423188e-04   3.22710770e-03   0.00000000e+00
   0.00000000e+00   0.00000000e+00   4.09247042e-01   3.79598776e-01
   0.00000000e+00   0.00000000e+00   3.10711365e-02   0.00000000e+00
   0.00000000e+00   2.49909888e-02   4.91075645e-03   0.00000000e+00
   1.11919418e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   4.77646514e-01   8.38618006e-01   5.53760960e-03   6.07781880e-02
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   2.01146601e-01   1.79025678e-01   2.21039705e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.89936181e-01
   0.00000000e+00   1.70923999e-01   0.00000000e+00   1.99189052e-01
   0.00000000e+00   0.00000000e+00   2.97798378e-01   5.77725290e-01
   6.98445958e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00
   2.52064320e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   8.28002361e-01   3.64595320e-01
   1.41655600e-02   2.39026006e-01   5.73875803e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00   1.27226463e-03   5.19739971e-01
   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.27644416e-01
   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.10698735e-01
   0.00000000e+00   4.12574652e-01   3.04071001e-01   1.52420185e-01
   0.00000000e+00   6.32799814e-01   1.17706069e-01   5.40902201e-01
   4.19072332e-01   1.18495779e-01   7.53851561e-01   6.56002713e-01
   7.22811671e-02   2.79826115e-01   8.04968629e-01]
Num of train docs per category:
 [  674   381   947   160  4179  2366   399  1930    49  1172   437   285
    76   246  1462  1205   142   202   793   190    62   922  1058   166
   196    41    43   443    31   120   312   286   343 10786   279   679
    94   187   167    17    65    12     8    34  1255   407   853    66
    43     0     3   449    40   641   102   400    54    15   166  3449
   363    49    35    59   138    38     2    45    52     2  6970  1133
   233  1004   293   106   172     6   197   471     0    13    90  1647
   166    92    37   913    23  1115   346   135    51  1294   732  1596
   943   699  2541  1508   311   606  5882]
Num of test docs per category:
 [ 6116  2652  9485  1946 39361 20624  2013 19512   438 11459  4947  2818
   623  1942 12956 10652  1131  1871  7141  1723   665  8137 10512  1085
  1785   238   464  3846   305  1178  2719  2480  3094 98023  2161  6567
   564  1669  1425   243   552    84    55   318 10807  3674  7246   591
   423    12    25  4266   510  5659   830  3449   491    98  1385 30336
  5683   958   594   625  2230   579    75   535  1140    19 60646  7564
  2234  9969  2249   987  1480    77  1568  4364     0   168   602 14870
  1487   721   542  8254   194  8513  4151   886   438 10872  5433 12079
  6430  5932 19619 10702  2733  5224 45829]


